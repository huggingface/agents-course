# Observabilidade e AvaliaÃ§Ã£o de Agentes de IA

## ğŸ” O que Ã© Observabilidade?

Observabilidade Ã© a capacidade de entender o que estÃ¡ acontecendo dentro do seu agente de IA analisando sinais externos, como logs, mÃ©tricas e traces. No contexto de agentes, isso significa acompanhar aÃ§Ãµes, uso de ferramentas, chamadas de modelo e respostas para depurar e melhorar o desempenho.

![Observability dashboard](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/bonus-unit2/langfuse-dashboard.png)

## ğŸ”­ Por que a observabilidade de agentes Ã© importante?

Sem observabilidade, agentes de IA sÃ£o "caixas-pretas". Ferramentas de observabilidade tornam os agentes transparentes, permitindo:

- Entender trade-offs entre custos e precisÃ£o  
- Medir latÃªncia  
- Detectar linguagem nociva e tentativas de prompt injection  
- Monitorar o feedback dos usuÃ¡rios

Em outras palavras, elas tornam seu agente de demonstraÃ§Ã£o pronto para produÃ§Ã£o!

## ğŸ”¨ Ferramentas de Observabilidade

Ferramentas comuns de observabilidade para agentes incluem plataformas como [Langfuse](https://langfuse.com) e [Arize](https://www.arize.com). Elas ajudam a coletar traces detalhados e oferecem dashboards para monitorar mÃ©tricas em tempo real, facilitando a detecÃ§Ã£o de problemas e a otimizaÃ§Ã£o de desempenho.

As ferramentas variam bastante em recursos e capacidades. Algumas sÃ£o open source, contam com comunidades grandes que influenciam seus roadmaps e oferecem integraÃ§Ãµes extensas. Outras sÃ£o especializadas em aspectos especÃ­ficos de LLMOps â€” observabilidade, avaliaÃ§Ãµes ou gerenciamento de prompts â€” enquanto algumas cobrem todo o fluxo de LLMOps. Explore a documentaÃ§Ã£o de cada opÃ§Ã£o para escolher a soluÃ§Ã£o que melhor se adapta Ã s suas necessidades.

Muitos frameworks de agentes, como o [smolagents](https://huggingface.co/docs/smolagents/v1.12.0/en/index), adotam o padrÃ£o [OpenTelemetry](https://opentelemetry.io/docs/) para expor metadados Ã s ferramentas de observabilidade. AlÃ©m disso, essas plataformas desenvolvem instrumentaÃ§Ãµes personalizadas para garantir flexibilidade no ritmo acelerado do ecossistema de LLMs. Consulte a documentaÃ§Ã£o da ferramenta que vocÃª usa para saber o que estÃ¡ disponÃ­vel.

## ğŸ”¬ Traces e Spans

Ferramentas de observabilidade costumam representar as execuÃ§Ãµes do agente como traces e spans.

- **Traces** representam uma tarefa completa do agente do inÃ­cio ao fim (como lidar com uma solicitaÃ§Ã£o de usuÃ¡rio).  
- **Spans** sÃ£o etapas individuais dentro de um trace (por exemplo, chamar um modelo de linguagem ou recuperar dados).

![Example of a smolagent trace in Langfuse](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/bonus-unit2/trace-tree.png)

## ğŸ“Š MÃ©tricas principais para monitorar

A seguir, algumas das mÃ©tricas mais comuns acompanhadas pelas ferramentas de observabilidade:

**LatÃªncia:** Com que rapidez o agente responde? Esperas longas prejudicam a experiÃªncia do usuÃ¡rio. MeÃ§a a latÃªncia de tarefas e etapas individuais rastreando as execuÃ§Ãµes. Por exemplo, se um agente leva 20 segundos para todas as chamadas ao modelo, talvez seja necessÃ¡rio usar um modelo mais rÃ¡pido ou paralelizar as chamadas.

**Custos:** Qual Ã© o gasto por execuÃ§Ã£o do agente? Agentes de IA dependem de chamadas a LLMs cobradas por token ou de APIs externas. Uso frequente de ferramentas ou mÃºltiplos prompts pode elevar os custos rapidamente. Se o agente chama o LLM cinco vezes para um ganho pequeno de qualidade, avalie se o custo vale a pena ou se Ã© possÃ­vel reduzir o nÃºmero de chamadas ou utilizar um modelo mais barato. Monitoramento em tempo real tambÃ©m ajuda a identificar picos inesperados (por exemplo, loops excessivos causados por bugs).

**Erros de requisiÃ§Ã£o:** Quantas chamadas falharam? Isso inclui erros de API ou falhas ao acionar ferramentas. Para tornar o agente mais robusto em produÃ§Ã£o, defina fallbacks ou novas tentativas. Ex.: se o provedor de LLM A estiver fora do ar, troque para o provedor B.

**Feedback do usuÃ¡rio:** AvaliaÃ§Ãµes diretas trazem insights valiosos, seja por ratings explÃ­citos (ğŸ‘/ğŸ‘, â­ de 1 a 5) ou por comentÃ¡rios textuais. Feedback negativo recorrente Ã© um alerta de que o agente nÃ£o estÃ¡ funcionando como deveria.

**Feedback implÃ­cito do usuÃ¡rio:** O comportamento do usuÃ¡rio gera sinais indiretos mesmo sem avaliaÃ§Ãµes explÃ­citas, como reformular perguntas logo em seguida, repetir a mesma consulta ou clicar em "tentar novamente". Se isso acontece com frequÃªncia, Ã© sinal de que o agente nÃ£o estÃ¡ atendendo Ã s expectativas.

**AcurÃ¡cia:** Com que frequÃªncia o agente produz respostas corretas ou desejÃ¡veis? A definiÃ§Ã£o de acurÃ¡cia varia (correÃ§Ã£o de problemas, precisÃ£o na recuperaÃ§Ã£o de informaÃ§Ãµes, satisfaÃ§Ã£o do usuÃ¡rio). O primeiro passo Ã© definir o que significa sucesso para o seu agente. Monitore a acurÃ¡cia por meio de verificaÃ§Ãµes automÃ¡ticas, notas de avaliaÃ§Ã£o ou rÃ³tulos de conclusÃ£o de tarefas, como marcar traces como "sucesso" ou "falha".

**MÃ©tricas de avaliaÃ§Ã£o automÃ¡tica:** VocÃª tambÃ©m pode configurar avaliaÃ§Ãµes automatizadas. Por exemplo, usar um LLM para pontuar a resposta do agente (se foi Ãºtil, precisa e assim por diante). Existem bibliotecas open source que ajudam a avaliar diferentes aspectos do agente, como [RAGAS](https://docs.ragas.io/) para agentes de RAG ou [LLM Guard](https://llm-guard.com/) para detectar linguagem nociva e prompt injection.

Na prÃ¡tica, combinar essas mÃ©tricas oferece a melhor cobertura sobre a "saÃºde" do agente. No [notebook de exemplo desta unidade](https://colab.research.google.com/#fileId=https://huggingface.co/agents-course/notebooks/blob/main/bonus-unit2/monitoring-and-evaluating-agents.ipynb), mostraremos como essas mÃ©tricas aparecem em cenÃ¡rios reais, mas antes vamos entender como se parece um fluxo tÃ­pico de avaliaÃ§Ã£o.

# ğŸ‘ Avaliando agentes de IA

A observabilidade nos dÃ¡ mÃ©tricas, mas avaliaÃ§Ã£o Ã© o processo de analisar esses dados (e realizar testes) para determinar quÃ£o bem o agente estÃ¡ atuando e como pode ser aprimorado. Em outras palavras, depois de coletar traces e mÃ©tricas, como usÃ¡-los para julgar o agente e tomar decisÃµes?

A avaliaÃ§Ã£o regular Ã© importante porque agentes de IA costumam ser nÃ£o determinÃ­sticos e podem evoluir (por atualizaÃ§Ãµes ou deriva de comportamento). Sem avaliaÃ§Ã£o, vocÃª nÃ£o saberia se o seu â€œagente espertoâ€ estÃ¡ realmente funcionando bem ou se regrediu.

Existem duas categorias de avaliaÃ§Ã£o para agentes: **avaliaÃ§Ã£o offline** e **avaliaÃ§Ã£o online**. Ambas sÃ£o valiosas e se complementam. Geralmente comeÃ§amos com a avaliaÃ§Ã£o offline, pois Ã© o passo mÃ­nimo antes de colocar qualquer agente em produÃ§Ã£o.

### ğŸ¥· AvaliaÃ§Ã£o offline

![Dataset items in Langfuse](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/bonus-unit2/example-dataset.png)

Ela consiste em avaliar o agente em um ambiente controlado, normalmente usando conjuntos de teste, nÃ£o consultas reais de usuÃ¡rios. VocÃª utiliza datasets curados, em que sabe qual Ã© a saÃ­da esperada ou o comportamento correto, e roda o agente sobre eles.

Por exemplo, se vocÃª criou um agente que resolve problemas matemÃ¡ticos em texto, pode ter um [dataset de teste](https://huggingface.co/datasets/gsm8k) com 100 questÃµes e respostas conhecidas. A avaliaÃ§Ã£o offline Ã© feita normalmente durante o desenvolvimento (e pode fazer parte do pipeline de CI/CD) para verificar melhorias e evitar regressÃµes. A vantagem Ã© que **ela Ã© repetÃ­vel e fornece mÃ©tricas claras de acurÃ¡cia, porque hÃ¡ ground truth**. VocÃª tambÃ©m pode simular consultas de usuÃ¡rios e comparar as respostas com gabaritos, ou usar mÃ©tricas automÃ¡ticas como as citadas acima.

O principal desafio da avaliaÃ§Ã£o offline Ã© garantir que o dataset de teste seja abrangente e se mantenha relevante â€” o agente pode se sair bem em um conjunto fixo, mas encontrar situaÃ§Ãµes muito diferentes em produÃ§Ã£o. Portanto, mantenha os conjuntos de teste atualizados com novos edge cases e exemplos que reflitam cenÃ¡rios reais. Uma combinaÃ§Ã£o de pequenos â€œsmoke testsâ€ com conjuntos maiores Ã© Ãºtil: os pequenos para verificaÃ§Ãµes rÃ¡pidas e os grandes para mÃ©tricas mais amplas.

### ğŸ”„ AvaliaÃ§Ã£o online

Ela ocorre em um ambiente real, durante o uso do agente em produÃ§Ã£o. Envolve monitorar o desempenho nas interaÃ§Ãµes com usuÃ¡rios reais e analisar continuamente os resultados.

VocÃª pode acompanhar taxas de sucesso, Ã­ndices de satisfaÃ§Ã£o ou outras mÃ©tricas em trÃ¡fego real. A vantagem Ã© que a avaliaÃ§Ã£o online **captura situaÃ§Ãµes que talvez nÃ£o apareÃ§am em ambiente controlado** â€” permite observar deriva do modelo ao longo do tempo e detectar consultas inesperadas que nÃ£o estavam no seu dataset de teste. Ela oferece uma imagem fiel de como o agente se comporta â€œno mundo realâ€.

AvaliaÃ§Ã£o online geralmente inclui coletar feedback implÃ­cito e explÃ­cito dos usuÃ¡rios, como jÃ¡ discutimos, alÃ©m de executar shadow tests ou testes A/B (em que uma nova versÃ£o roda em paralelo com a antiga para comparaÃ§Ã£o). O desafio Ã© conseguir rÃ³tulos ou notas confiÃ¡veis nas interaÃ§Ãµes ao vivo â€” muitas vezes vocÃª dependerÃ¡ do feedback dos usuÃ¡rios ou de mÃ©tricas indiretas (ex.: o usuÃ¡rio clicou no resultado?).

### ğŸ¤ Combinando as duas abordagens

Na prÃ¡tica, avaliar bem um agente envolve misturar mÃ©todos **online** e **offline**. VocÃª pode executar benchmarks offline regulares para mensurar o desempenho em tarefas definidas e, ao mesmo tempo, monitorar o uso ao vivo para captar o que os testes nÃ£o mostram. Por exemplo, testes offline podem indicar se a taxa de acerto de um agente de geraÃ§Ã£o de cÃ³digo estÃ¡ melhorando, enquanto o monitoramento online pode alertar que usuÃ¡rios comeÃ§aram a fazer perguntas de uma categoria com a qual o agente tem dificuldade. Juntas, as duas abordagens fornecem um retrato mais robusto.

Muitas equipes adotam um ciclo contÃ­nuo: _avaliaÃ§Ã£o offline â†’ deploy de nova versÃ£o do agente â†’ monitoramento online para coletar novos exemplos de falha â†’ inclusÃ£o desses exemplos no conjunto offline â†’ iterar_. Assim, a avaliaÃ§Ã£o se torna constante e cada vez melhor.

## ğŸ§‘â€ğŸ’» Vamos ver isso na prÃ¡tica

Na prÃ³xima seÃ§Ã£o, veremos exemplos de como usar ferramentas de observabilidade para monitorar e avaliar o nosso agente.

