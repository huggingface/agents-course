# Vamos Fazer Fine-Tuning do Seu Modelo para Function Calling

Estamos prontos para ajustar nosso primeiro modelo para function calling üî•.

## Como treinamos nosso modelo para function calling?

> Resposta: precisamos de **dados**

O processo de treinamento de um modelo pode ser dividido em 3 etapas:

1. **O modelo √© pr√©-treinado em uma grande quantidade de dados**. O resultado dessa etapa √© um **modelo pr√©-treinado**. Por exemplo, [google/gemma-2-2b](https://huggingface.co/google/gemma-2-2b). √â um modelo base e sabe apenas **prever o pr√≥ximo token sem grandes capacidades de seguir instru√ß√µes**.

2. Para ser √∫til em um contexto de chat, o modelo precisa ent√£o ser **ajustado (fine-tuned)** para seguir instru√ß√µes. Nessa etapa, ele pode ser treinado pelos criadores, pela comunidade open-source, por voc√™ ou por qualquer pessoa. Por exemplo, [google/gemma-2-2b-it](https://huggingface.co/google/gemma-2-2b-it) √© um modelo ajustado para instru√ß√µes pela equipe do projeto Gemma, do Google.

3. O modelo pode ent√£o ser **alinhado** √†s prefer√™ncias do criador. Pense, por exemplo, em um modelo de atendimento ao cliente que jamais deve ser grosseiro.

Produtos completos como Gemini ou Mistral **passam pelas 3 etapas**, enquanto os modelos dispon√≠veis no Hugging Face podem ter completado uma ou mais dessas fases.

Neste tutorial, vamos construir um modelo com function calling a partir do [google/gemma-2-2b-it](https://huggingface.co/google/gemma-2-2b-it). Escolhemos o modelo j√° ajustado [google/gemma-2-2b-it](https://huggingface.co/google/gemma-2-2b-it) em vez do modelo base [google/gemma-2-2b](https://huggingface.co/google/gemma-2-2b) porque ele j√° foi melhorado para o nosso caso de uso.

Partir do modelo apenas pr√©-treinado **exigiria muito mais treinamento para aprender a seguir instru√ß√µes, conversar E executar function calling**.

Ao come√ßar pelo modelo ajustado para instru√ß√µes, **minimizamos a quantidade de informa√ß√£o que o nosso modelo precisa aprender**.

## LoRA (Low-Rank Adaptation of Large Language Models)

LoRA √© uma t√©cnica de treinamento popular e leve que **reduz significativamente o n√∫mero de par√¢metros trein√°veis**.

Ela funciona **inserindo um pequeno conjunto de novos pesos como um adaptador dentro do modelo para serem treinados**. Isso torna o treinamento com LoRA muito mais r√°pido, eficiente em mem√≥ria e gera pesos menores (algumas centenas de MB), f√°ceis de armazenar e compartilhar.

<img src="https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/unit1/blog_multi-lora-serving_LoRA.gif" alt="LoRA inference" width="50%"/>

LoRA atua adicionando pares de matrizes de decomposi√ß√£o de posto baixo √†s camadas do Transformer, normalmente focando nas camadas lineares. Durante o treinamento, ‚Äúcongelamos‚Äù o restante do modelo e atualizamos apenas os pesos desses adaptadores rec√©m-adicionados.

Ao fazer isso, o n√∫mero de **par√¢metros** que precisamos treinar cai bastante, j√° que apenas os pesos dos adaptadores s√£o atualizados.

Durante a infer√™ncia, a entrada passa tanto pelo adaptador quanto pelo modelo base; tamb√©m √© poss√≠vel fundir esses pesos com o modelo base, sem gerar lat√™ncia adicional.

LoRA √© especialmente √∫til para adaptar modelos de linguagem **grandes** a tarefas ou dom√≠nios espec√≠ficos mantendo os requisitos de recursos sob controle. Isso ajuda a reduzir a mem√≥ria **necess√°ria** para treinar um modelo.

Se quiser aprender mais sobre como LoRA funciona, confira este [tutorial](https://huggingface.co/learn/nlp-course/chapter11/4?fw=pt).

## Fazendo Fine-Tuning de um Modelo para Function Calling

Voc√™ pode acessar o notebook do tutorial üëâ [aqui](https://huggingface.co/agents-course/notebooks/blob/main/bonus-unit1/bonus-unit1.ipynb).

Depois, clique em [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/#fileId=https://huggingface.co/agents-course/notebooks/blob/main/bonus-unit1/bonus-unit1.ipynb) para execut√°-lo em um Notebook Colab.

