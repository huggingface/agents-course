# Introdu√ß√£o

![Bonus Unit 1 Thumbnail](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/bonus-unit1/thumbnail.jpg)

Bem-vindo a esta primeira **Unidade B√¥nus**, na qual voc√™ vai aprender a **fazer fine-tuning de um Large Language Model (LLM) para chamadas de fun√ß√£o**.

Quando falamos de LLMs, function calling est√° rapidamente se tornando uma t√©cnica indispens√°vel.

A ideia √© que, em vez de depender apenas de abordagens baseadas em prompt como fizemos na Unidade 1, o function calling treina seu modelo para **tomar a√ß√µes e interpretar observa√ß√µes durante a fase de treinamento**, deixando sua IA mais robusta.

> **Quando devo fazer esta Unidade B√¥nus?**
>
> Esta se√ß√£o √© **opcional** e mais avan√ßada do que a Unidade 1, ent√£o sinta-se √† vontade para faz√™-la agora ou revisit√°-la quando seu conhecimento tiver evolu√≠do com este curso.
>
> Mas n√£o se preocupe, esta Unidade B√¥nus foi concebida para trazer todas as informa√ß√µes necess√°rias. Vamos gui√°-lo por cada conceito essencial de fine-tuning para chamadas de fun√ß√£o, mesmo que voc√™ ainda n√£o domine todos os detalhes desse processo.

A melhor forma de aproveitar esta Unidade B√¥nus √©:

1. Saber como fazer fine-tuning de um LLM com Transformers; se ainda n√£o souber, [confira isto](https://huggingface.co/learn/nlp-course/chapter3/1?fw=pt).
2. Conhecer o `SFTTrainer` para ajustar o modelo; para saber mais, [acesse esta documenta√ß√£o](https://huggingface.co/learn/nlp-course/en/chapter11/1).

---

## O que voc√™ vai aprender

1. **Function Calling**  
   Como LLMs modernos estruturam suas conversas, permitindo acionar **Ferramentas**.

2. **LoRA (Low-Rank Adaptation)**  
   Um m√©todo de fine-tuning **leve e eficiente** que reduz o custo computacional e de armazenamento. LoRA torna o treinamento de grandes modelos *mais r√°pido, barato e f√°cil* de colocar em produ√ß√£o.

3. **O ciclo Pensar ‚Üí Agir ‚Üí Observar** em modelos com function calling  
   Uma abordagem simples e poderosa para estruturar como o seu modelo decide quando (e como) chamar fun√ß√µes, acompanhar etapas intermedi√°rias e interpretar resultados vindos de ferramentas externas ou APIs.

4. **Novos tokens especiais**  
   Vamos apresentar **marcadores especiais** que ajudam o modelo a diferenciar entre:
   - racioc√≠nio interno (‚Äúchain-of-thought‚Äù);  
   - chamadas de fun√ß√£o enviadas;  
   - respostas que retornam de ferramentas externas.

---

Ao final desta unidade b√¥nus, voc√™ ser√° capaz de:

- **Entender** o funcionamento interno de APIs no contexto de Ferramentas;  
- **Fazer fine-tuning** de um modelo usando a t√©cnica LoRA;  
- **Implementar** e **ajustar** o ciclo Pensar ‚Üí Agir ‚Üí Observar para criar fluxos de trabalho robustos e f√°ceis de manter para function calling;  
- **Criar e utilizar** tokens especiais para separar de forma fluida o racioc√≠nio interno das a√ß√µes externas do modelo.

E voc√™ **ter√° ajustado seu pr√≥prio modelo para realizar chamadas de fun√ß√£o.** üî•

Vamos mergulhar em **function calling**!
