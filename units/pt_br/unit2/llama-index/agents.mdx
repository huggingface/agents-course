# Usando agentes no LlamaIndex

Lembra do Alfred, nosso mordomo-agente? Ele está prestes a receber um upgrade!
Agora que entendemos as ferramentas disponíveis no LlamaIndex, podemos ampliar as capacidades do Alfred.

Antes, vamos relembrar o que caracteriza um agente. Na Unidade 1 aprendemos que:

> An Agent is a system that leverages an AI model to interact with its environment to achieve a user-defined objective. It combines reasoning, planning, and action execution (often via external tools) to fulfil tasks.

O LlamaIndex oferece **três tipos principais de agentes de raciocínio:**

![Agents](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/unit2/llama-index/agents.png)

1. `Function Calling Agents` — utilizam modelos com suporte a function calling.  
2. `ReAct Agents` — funcionam com qualquer modelo (chat ou completions) e são bons em tarefas complexas de raciocínio.  
3. `Advanced Custom Agents` — empregam métodos avançados para lidar com cenários e workflows mais elaborados.

> [!TIP]
> Find more information on advanced agents on <a href="https://github.com/run-llama/llama_index/blob/main/llama-index-core/llama_index/core/agent/workflow/base_agent.py">BaseWorkflowAgent</a>

## Inicializando agentes

> [!TIP]
> You can follow the code in <a href="https://huggingface.co/agents-course/notebooks/blob/main/unit2/llama-index/agents.ipynb" target="_blank">this notebook</a> that you can run using Google Colab.

Para criar um agente, fornecemos **um conjunto de funções/ferramentas que define suas capacidades**.  
Veja como montar um agente com ferramentas básicas. No momento, o LlamaIndex escolhe automaticamente entre function calling (se o modelo suportar) ou o loop padrão ReAct.

Modelos com API de ferramentas/funções são relativamente novos, mas poderosos: dispensam prompts complexos e permitem que o LLM monte chamadas seguindo schemas definidos.

Agentes ReAct também lidam bem com tarefas complexas e funcionam com qualquer LLM (chat ou texto), exibindo inclusive o raciocínio seguido.

```python
from llama_index.llms.huggingface_api import HuggingFaceInferenceAPI
from llama_index.core.agent.workflow import AgentWorkflow
from llama_index.core.tools import FunctionTool

# define sample Tool -- type annotations, function names, and docstrings, are all included in parsed schemas!
def multiply(a: int, b: int) -> int:
    """Multiplies two integers and returns the resulting integer"""
    return a * b

# initialize llm
llm = HuggingFaceInferenceAPI(model_name="Qwen/Qwen2.5-Coder-32B-Instruct")

# initialize agent
agent = AgentWorkflow.from_tools_or_functions(
    [FunctionTool.from_defaults(multiply)],
    llm=llm
)
```

**Agentes são stateless por padrão**, mas podem lembrar interações passadas usando um objeto `Context`.  
Isso é útil em chatbots com memória ou gestores de tarefas que precisam acompanhar progresso.

```python
# stateless
response = await agent.run("What is 2 times 2?")

# remembering state
from llama_index.core.workflow import Context

ctx = Context(agent)

response = await agent.run("My name is Bob.", ctx=ctx)
response = await agent.run("What was my name again?", ctx=ctx)
```

Note que agentes no `LlamaIndex` são assíncronos, pois usam `await`. Se precisar de reforço em async, consulte o [guia oficial](https://docs.llamaindex.ai/en/stable/getting_started/async_python/).

Agora que dominamos o básico, vamos usar ferramentas mais avançadas.

## Creating RAG Agents with QueryEngineTools

**Agentic RAG é uma abordagem poderosa para responder perguntas com seus dados.**  
Podemos entregar várias ferramentas ao Alfred para auxiliá-lo. Em vez de sempre consultar documentos, ele pode decidir usar outros fluxos ou ferramentas.

![Agentic RAG](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/unit2/llama-index/agentic-rag.png)

É simples **transformar o `QueryEngine` em ferramenta**.  
Ao fazer isso, precisamos **definir nome e descrição**, para que o LLM use o recurso da forma correta. Veja o exemplo com o `QueryEngine` criado na [seção de componentes](components):

```python
from llama_index.core.tools import QueryEngineTool

query_engine = index.as_query_engine(llm=llm, similarity_top_k=3) # as shown in the Components in LlamaIndex section

query_engine_tool = QueryEngineTool.from_defaults(
    query_engine=query_engine,
    name="name",
    description="a specific description",
    return_direct=False,
)
query_engine_agent = AgentWorkflow.from_tools_or_functions(
    [query_engine_tool],
    llm=llm,
    system_prompt="You are a helpful assistant that has access to a database containing persona descriptions. "
)
```

## Criando sistemas multiagentes

A classe `AgentWorkflow` suporta sistemas multiagentes. Basta dar nome e descrição a cada agente: o workflow mantém um “orador” ativo e permite handoff entre agentes.

Ao restringir o escopo de cada um, aumentamos a precisão nas respostas.

**Agentes no LlamaIndex também podem ser utilizados como ferramentas** por outros agentes — ideal para cenários complexos.

```python
from llama_index.core.agent.workflow import (
    AgentWorkflow,
    FunctionAgent,
    ReActAgent,
)

# Define some tools
def add(a: int, b: int) -> int:
    """Add two numbers."""
    return a + b


def subtract(a: int, b: int) -> int:
    """Subtract two numbers."""
    return a - b


# Create agent configs
# NOTE: we can use FunctionAgent or ReActAgent here.
# FunctionAgent works for LLMs with a function calling API.
# ReActAgent works for any LLM.
calculator_agent = ReActAgent(
    name="calculator",
    description="Performs basic arithmetic operations",
    system_prompt="You are a calculator assistant. Use your tools for any math operation.",
    tools=[add, subtract],
    llm=llm,
)

query_agent = ReActAgent(
    name="info_lookup",
    description="Looks up information about XYZ",
    system_prompt="Use your tool to query a RAG system to answer information about XYZ",
    tools=[query_engine_tool],
    llm=llm
)

# Create and run the workflow
agent = AgentWorkflow(
    agents=[calculator_agent, query_agent], root_agent="calculator"
)

# Run the system
response = await agent.run(user_msg="Can you add 5 and 3?")
```

> [!TIP]
> Quer saber mais? Confira a <a href="https://docs.llamaindex.ai/en/stable/examples/agent/agent_workflow_basic/">introdução ao AgentWorkflow</a> ou o <a href="https://docs.llamaindex.ai/en/stable/understanding/agent/">guia de agentes</a> — há conteúdo sobre streaming, serialização de contexto e human-in-the-loop.

Agora que entendemos agentes e ferramentas, vamos usar o LlamaIndex para **criar workflows configuráveis e fáceis de manejar!**
