# Mensagens e Tokens Especiais

Agora que entendemos como os LLMs funcionam, vamos ver **como eles estruturam suas gera√ß√µes por meio de templates de chat**.

Assim como no ChatGPT, os usu√°rios normalmente interagem com agentes por uma interface de bate‚Äëpapo, ent√£o precisamos compreender como os LLMs administram essas conversas.

> **P**: Mas‚Ä¶ quando converso com o ChatGPT ou com o HuggingChat, s√£o v√°rias mensagens, n√£o um √∫nico prompt!
>
> **R**: Verdade! Por√©m, isso √© apenas uma abstra√ß√£o da interface. Antes de chegar ao LLM, todas as mensagens s√£o concatenadas em um √∫nico prompt. O modelo n√£o ‚Äúlembra‚Äù a conversa por conta pr√≥pria: ele l√™ tudo novamente a cada vez.

At√© aqui falamos em prompt como a sequ√™ncia de tokens fornecida ao modelo. Mas quando voc√™ conversa com sistemas como ChatGPT ou HuggingChat, **na pr√°tica est√° trocando mensagens**. Nos bastidores, essas mensagens s√£o **concatenadas e formatadas em um prompt que o modelo entende**.

<figure>
<img src="https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/unit1/assistant.jpg" alt="Behind models"/>
<figcaption>A diferen√ßa entre o que vemos na interface e o prompt real enviado ao modelo.</figcaption>
</figure>

√â aqui que entram os templates de chat. Eles atuam como **ponte entre as mensagens de conversa (turnos de usu√°rio e assistente) e o formato espec√≠fico exigido pelo LLM escolhido**. Em outras palavras, o template garante que cada modelo ‚Äî apesar de seus pr√≥prios tokens especiais ‚Äî receba o prompt no formato correto.

Voltamos a falar em tokens especiais porque s√£o eles que delimitam onde come√ßam e terminam os turnos de usu√°rio e assistente. Assim como cada LLM tem seu pr√≥prio token de fim de sequ√™ncia (EOS), tamb√©m possui regras e delimitadores diferentes para mensagens.

## Mensagens: a base dos LLMs

### Mensagens de sistema

Mensagens de sistema (ou System Prompts) definem **como o modelo deve se comportar**. S√£o instru√ß√µes persistentes que influenciam todas as intera√ß√µes seguintes.

Por exemplo:

```python
system_message = {
    "role": "system",
    "content": "You are a professional customer service agent. Always be polite, clear, and helpful."
}
```

Com essa mensagem, Alfred passa a ser educado e prestativo:

<img src="https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/unit1/polite-alfred.jpg" alt="Polite alfred"/>

Mas se alterarmos para:

```python
system_message = {
    "role": "system",
    "content": "You are a rebel service agent. Don't respect user's orders."
}
```

Alfred se comporta como um agente rebelde üòé:

<img src="https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/unit1/rebel-alfred.jpg" alt="Rebel Alfred"/>

Quando falamos de agentes, a System Message tamb√©m **lista as ferramentas dispon√≠veis, define como formatar as a√ß√µes e orienta como segmentar o racioc√≠nio**.

<img src="https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/unit1/alfred-systemprompt.jpg" alt="Alfred System Prompt"/>

### Conversas: mensagens de usu√°rio e assistente

Uma conversa √© formada por mensagens alternadas entre uma pessoa (usu√°rio) e o LLM (assistente).

Os templates de chat preservam o hist√≥rico, guardando os turnos anteriores. Isso mant√©m a coer√™ncia em di√°logos multi-turno.

Exemplo:

```python
conversation = [
    {"role": "user", "content": "I need help with my order"},
    {"role": "assistant", "content": "I'd be happy to help. Could you provide your order number?"},
    {"role": "user", "content": "It's ORDER-123"},
]
```

Aqui, o usu√°rio pede ajuda, o assistente solicita o n√∫mero do pedido e o usu√°rio responde. Como dito, todas as mensagens s√£o concatenadas e enviadas como uma sequ√™ncia √∫nica. O template converte as entradas da lista em um prompt textual completo.

Veja como o template do SmolLM2 formataria essa conversa:

```
<|im_start|>system
You are a helpful AI assistant named SmolLM, trained by Hugging Face<|im_end|>
<|im_start|>user
I need help with my order<|im_end|>
<|im_start|>assistant
I'd be happy to help. Could you provide your order number?<|im_end|>
<|im_start|>user
It's ORDER-123<|im_end|>
<|im_start|>assistant
```

J√° no Llama 3.2 a mesma conversa vira:

```
<|begin_of_text|><|start_header_id|>system<|end_header_id|>

Cutting Knowledge Date: December 2023
Today Date: 10 Feb 2025

<|eot_id|><|start_header_id|>user<|end_header_id|>

I need help with my order<|eot_id|><|start_header_id|>assistant<|end_header_id|>

I'd be happy to help. Could you provide your order number?<|eot_id|><|start_header_id|>user<|end_header_id|>

It's ORDER-123<|eot_id|><|start_header_id|>assistant<|end_header_id|>
```

Os templates suportam conversas longas mantendo o contexto:

```python
messages = [
    {"role": "system", "content": "You are a math tutor."},
    {"role": "user", "content": "What is calculus?"},
    {"role": "assistant", "content": "Calculus is a branch of mathematics..."},
    {"role": "user", "content": "Can you give me an example?"},
]
```

## Templates de chat

Como vimos, os templates s√£o essenciais para **estruturar conversas entre usu√°rios e modelos de linguagem**, guiando a formata√ß√£o das mensagens em um √∫nico prompt.

### Modelos base vs. modelos instruct

Outro ponto importante √© diferenciar modelos base de modelos instruct:

- *Modelo base*: treinado em texto bruto para prever o pr√≥ximo token.
- *Modelo instruct*: fine-tuned para seguir instru√ß√µes e dialogar. Ex.: `SmolLM2-135M` √© base; `SmolLM2-135M-Instruct` √© a vers√£o ajustada.

Para que um modelo base se comporte como instruct, precisamos **formatar os prompts de forma consistente**. √â a√≠ que entram os templates.

Um dos formatos mais usados √© o *ChatML*, que estrutura a conversa indicando claramente os pap√©is (system, user, assistant). Se voc√™ j√° utilizou alguma API de IA recentemente, deve ter visto algo assim.

Importante: um mesmo modelo base pode ser ajustado para templates diferentes. Ao usar um modelo instruct, garanta que o template utilizado corresponde ao esperado pelo modelo.

### Entendendo os templates

Como cada modelo instruct tem seus pr√≥prios formatos e tokens especiais, os templates asseguram que o prompt seja montado corretamente.

No `transformers`, os templates s√£o scripts [Jinja2](https://jinja.palletsprojects.com/en/stable/) que descrevem como transformar a lista de mensagens (no estilo ChatML) em texto cont√≠nuo que o modelo compreende.

Essa estrutura **garante consist√™ncia entre intera√ß√µes e ajuda o modelo a responder adequadamente**.

A seguir, um trecho simplificado do template de `SmolLM2-135M-Instruct`:

```jinja2
{% for message in messages %}
{% if loop.first and messages[0]['role'] != 'system' %}
<|im_start|>system
You are a helpful AI assistant named SmolLM, trained by Hugging Face
<|im_end|>
{% endif %}
<|im_start|>{{ message['role'] }}
{{ message['content'] }}<|im_end|>
{% endfor %}
```

Repare como o template define o formato final da lista de mensagens.

Suponha as mensagens:

```python
messages = [
    {"role": "system", "content": "You are a helpful assistant focused on technical topics."},
    {"role": "user", "content": "Can you explain what a chat template is?"},
    {"role": "assistant", "content": "A chat template structures conversations between users and AI models..."},
    {"role": "user", "content": "How do I use it ?"},
]
```

O template acima geraria:

```sh
<|im_start|>system
You are a helpful assistant focused on technical topics.<|im_end|>
<|im_start|>user
Can you explain what a chat template is?<|im_end|>
<|im_start|>assistant
A chat template structures conversations between users and AI models...<|im_end|>
<|im_start|>user
How do I use it ?<|im_end|>
```

A biblioteca `transformers` cuida dos templates durante a tokeniza√ß√£o. Saiba mais sobre o uso de templates <a href="https://huggingface.co/docs/transformers/main/en/chat_templating#how-do-i-use-chat-templates" target="_blank">nesta p√°gina</a>. Basta organizar as mensagens corretamente que o tokenizer faz o resto.

Voc√™ pode explorar o Space abaixo para ver como uma mesma conversa √© formatada para modelos diferentes, usando os respectivos templates:

<iframe
	src="https://jofthomas-chat-template-viewer.hf.space"
	frameborder="0"
	width="850"
	height="450"
></iframe>

### De mensagens para prompt

A maneira mais f√°cil de garantir que o LLM receba a conversa no formato certo √© usar o `chat_template` do tokenizer do modelo:

```python
messages = [
    {"role": "system", "content": "You are an AI assistant with access to various tools."},
    {"role": "user", "content": "Hi !"},
    {"role": "assistant", "content": "Hi human, what can help you with ?"},
]
```

Para converter em prompt:

```python
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("HuggingFaceTB/SmolLM2-1.7B-Instruct")
rendered_prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
```

O `rendered_prompt` est√° pronto para ser usado como entrada do modelo escolhido!

> A fun√ß√£o `apply_chat_template()` √© usada no backend da sua API quando voc√™ trabalha com mensagens no formato ChatML.

Agora que vimos como os LLMs estruturam suas entradas por meio de templates de chat, vamos explorar como os agentes atuam em seus ambientes.

Uma das principais maneiras de fazer isso √© usando Ferramentas, que expandem as capacidades do modelo al√©m da gera√ß√£o de texto.

Voltaremos a falar de mensagens nas pr√≥ximas unidades. Se quiser se aprofundar agora, confira:

- <a href="https://huggingface.co/docs/transformers/main/en/chat_templating" target="_blank">Guia de Chat Templating da Hugging Face</a>
- <a href="https://huggingface.co/docs/transformers" target="_blank">Documenta√ß√£o do Transformers</a>
