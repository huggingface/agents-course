<CourseFloatingBanner chapter={2}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/#fileId=https%3A//huggingface.co/agents-course/notebooks/blob/main/fr/bonus-unit2/monitoring-and-evaluating-agents.ipynb"},
]} />

# Observer et √©valuer des agents

> [!TIP]
> Vous pouvez suivre le code dans <a href="https://colab.research.google.com/#fileId=https%3A//huggingface.co/agents-course/notebooks/blob/main/fr/bonus-unit2/monitoring-and-evaluating-agents.ipynb" target="_blank">ce <i>notebook</i></a> que vous pouvez ex√©cuter avec Google Colab.

Dans ce *notebook*, nous apprendrons comment **surveiller les √©tapes internes (traces) de notre agent** et **√©valuer ses performances** en utilisant des outils d'observabilit√© *open source*.

La capacit√© d'observer et d'√©valuer le comportement d'un agent est essentielle pour :
- D√©boguer les probl√®mes lorsque les t√¢ches √©chouent ou produisent des r√©sultats sous-optimaux
- Contr√¥ler les co√ªts et les performances en temps r√©el
- Am√©liorer la fiabilit√© et la s√©curit√© gr√¢ce √† un retour d'information continu

## Pr√©requis de l'exercice üèóÔ∏è

Avant d'ex√©cuter ce *notebook*, assurez-vous d'avoir :

üî≤ üìö **Etudier la section [Introduction aux agents](https://huggingface.co/learn/agents-course/fr/unit1/introduction)**

üî≤ üìö **Etudier la section [le *framework* smolagents](https://huggingface.co/learn/agents-course/fr/unit2/smolagents/introduction)**

## √âtape 0 : Installer les biblioth√®ques n√©cessaires

Nous aurons besoin de quelques biblioth√®ques qui nous permettront d'ex√©cuter, de contr√¥ler et d'√©valuer nos agents :

```python
%pip install langfuse 'smolagents[telemetry]' openinference-instrumentation-smolagents datasets 'smolagents[gradio]' gradio --upgrade
```

## √âtape 1 : Instrumenter votre agent

Dans ce *notebook*, nous utiliserons [Langfuse](https://langfuse.com/) comme outil d'observabilit√©, mais vous pouvez utiliser **n'importe quel autre service compatible avec OpenTelemetry**. Le code ci-dessous montre comment d√©finir les variables d'environnement pour Langfuse (ou n'importe quel *endpoint OTel*) et comment instrumenter votre smolagent.

**Note :** Si vous utilisez LlamaIndex ou LangGraph, vous pouvez trouver de la documentation sur leur instrumentation [ici](https://langfuse.com/docs/integrations/llama-index/workflows) et [ici](https://langfuse.com/docs/integrations/langchain/example-python-langgraph).

D'abord, configurons les credentials *Langfuse* comme variables d'environnement. Obtenez vos cl√©s API *Langfuse* en vous inscrivant sur [Langfuse Cloud](https://cloud.langfuse.com) ou en [auto-h√©bergeant Langfuse](https://langfuse.com/self-hosting).

```python
import os
# Obtenez les cl√©s pour votre projet depuis la page des param√®tres du projet : https://cloud.langfuse.com
os.environ["LANGFUSE_PUBLIC_KEY"] = "pk-lf-..." 
os.environ["LANGFUSE_SECRET_KEY"] = "sk-lf-..." 
os.environ["LANGFUSE_HOST"] = "https://cloud.langfuse.com" # üá™üá∫ r√©gion EU
# os.environ["LANGFUSE_HOST"] = "https://us.cloud.langfuse.com" # üá∫üá∏ r√©gion US
```
Nous devons aussi configurer notre token *Hugging Face* pour les appels d'inf√©rence.

```python
# D√©finissez vos tokens/secrets Hugging Face comme variable d'environnement
os.environ["HF_TOKEN"] = "hf_..." 
```

Avec les variables d'environnement d√©finies, nous pouvons maintenant initialiser le client *Langfuse*. `get_client()` initialise le client *Langfuse* en utilisant les credentials fournis dans les variables d'environnement.

```python
from langfuse import get_client
 
langfuse = get_client()
 
# V√©rifier la connexion
if langfuse.auth_check():
    print("Le client Langfuse est authentifi√© et pr√™t !")
else:
    print("L'authentification a √©chou√©. Veuillez v√©rifier vos credentials et h√¥te.")
```

Ensuite, nous pouvons configurer le `SmolagentsInstrumentor()` pour instrumenter notre *smolagent* et envoyer des traces √† *Langfuse*.

```python
from openinference.instrumentation.smolagents import SmolagentsInstrumentor
 
SmolagentsInstrumentor().instrument()
```

## √âtape 2 : Testez votre instrumentation

Voici un simple *CodeAgent* de smolagents qui calcule `1+1`. Nous l'ex√©cutons pour confirmer que l'instrumentation fonctionne correctement. Si tout est configur√© correctement, vous verrez des logs/spans dans votre tableau de bord d'observabilit√©.

```python
from smolagents import InferenceClientModel, CodeAgent

# Cr√©er un agent basique pour tester l'instrumentation
agent = CodeAgent(
    tools=[],
    model=InferenceClientModel()
)

agent.run("1+1=")
```

Consultez votre [Langfuse Traces Dashboard](https://cloud.langfuse.com/traces) (ou l'outil d'observabilit√© de votre choix) pour confirmer que les port√©es et les logs ont √©t√© enregistr√©s.

Exemple de capture d'√©cran de Langfuse :

![Example trace in Langfuse](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/bonus-unit2/first-example-trace.png)

_[Lien vers la trace](https://cloud.langfuse.com/project/cloramnkj0002jz088vzn1ja4/traces/1b94d6888258e0998329cdb72a371155?timestamp=2025-03-10T11%3A59%3A41.743Z)_

## √âtape 3 : Observer et √©valuer un agent plus complexe

Maintenant que vous avez confirm√© que votre instrumentation fonctionne, essayons une requ√™te plus complexe afin de voir comment les mesures avanc√©es (utilisation des *tokens*, latence, co√ªts, etc.) sont suivies.

```python
from smolagents import (CodeAgent, DuckDuckGoSearchTool, InferenceClientModel)

search_tool = DuckDuckGoSearchTool()
agent = CodeAgent(tools=[search_tool], model=InferenceClientModel())

agent.run("Combien de Rubik's Cubes pourrait-on faire tenir dans la Cath√©drale Notre-Dame ?")
```

### Structure de la trace

La plupart des outils d'observabilit√© enregistrent une **trace** qui contient des **spans**, qui repr√©sentent chaque √©tape de la logique de votre agent. Ici, la trace contient l'ex√©cution globale de l'agent et les sous-p√©riodes pour :
- les appels √† l'outil (DuckDuckGoSearchTool)
- Les appels LLM (InferenceClientModel)

Vous pouvez les inspecter pour voir pr√©cis√©ment o√π le temps est pass√©, combien de *tokens* sont utilis√©s, etc. :

![Trace tree in Langfuse](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/bonus-unit2/trace-tree.png)

_[Lien vers la trace](https://cloud.langfuse.com/project/cloramnkj0002jz088vzn1ja4/traces/1ac33b89ffd5e75d4265b62900c348ed?timestamp=2025-03-07T13%3A45%3A09.149Z&display=preview)_

## √âvaluation en ligne

Dans la section pr√©c√©dente, nous avons appris la diff√©rence entre l'√©valuation en ligne et hors ligne. Nous allons maintenant voir comment surveiller votre agent en production et l'√©valuer en direct.

### M√©triques courantes √† suivre en production

1. **Co√ªts** - L'instrumentation smolagents capture l'utilisation des *tokens*, que vous pouvez transformer en co√ªts approximatifs en assignant un prix par *token*.
2. **Latence** - Observez le temps n√©cessaire √† la r√©alisation de chaque √©tape ou de l'ensemble de l'ex√©cution.
3. **Retour utilisateur** - Les utilisateurs peuvent fournir un retour direct (pouce vers le haut/vers le bas) pour aider √† affiner ou √† corriger l'agent.
4. ***LLM-as-a-Judge*** - Utilisez un autre LLM pour √©valuer les r√©sultats de votre agent en quasi temps r√©el (par exemple, v√©rification de la toxicit√© ou de l'exactitude des r√©sultats).

Ci-dessous, nous montrons des exemples de ces m√©triques.

#### 1. Co√ªts

Vous trouverez ci-dessous une capture d'√©cran montrant l'utilisation des appels `Qwen2.5-Coder-32B-Instruct`. Ceci est utile pour voir les √©tapes co√ªteuses et optimiser votre agent.

![Costs](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/bonus-unit2/smolagents-costs.png)

_[Lien vers la trace](https://cloud.langfuse.com/project/cloramnkj0002jz088vzn1ja4/traces/1ac33b89ffd5e75d4265b62900c348ed?timestamp=2025-03-07T13%3A45%3A09.149Z&display=preview)_

#### 2. Temps de latence

Nous pouvons √©galement voir combien de temps a dur√© chaque √©tape. Dans l'exemple ci-dessous, l'ensemble de la conversation a dur√© 32 secondes, que vous pouvez r√©partir par √©tape. Cela vous permet d'identifier les goulets d'√©tranglement et d'optimiser votre agent.

![Latency](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/bonus-unit2/smolagents-latency.png)

_[Lien vers la trace](https://cloud.langfuse.com/project/cloramnkj0002jz088vzn1ja4/traces/1ac33b89ffd5e75d4265b62900c348ed?timestamp=2025-03-07T13%3A45%3A09.149Z&display=preview)_blank

#### 3. Attributs suppl√©mentaires

Vous pouvez √©galement passer des attributs suppl√©mentaires √† vos spans. Ceux-ci peuvent inclure `user_id`, `tags`, `session_id`, et des m√©tadonn√©es personnalis√©es. Enrichir les traces avec ces d√©tails est important pour l'analyse, le d√©bogage et la surveillance du comportement de votre application √† travers diff√©rents utilisateurs ou sessions.

```python
from smolagents import (CodeAgent, DuckDuckGoSearchTool, InferenceClientModel)

search_tool = DuckDuckGoSearchTool()
agent = CodeAgent(
    tools=[search_tool],
    model=InferenceClientModel()
)

with langfuse.start_as_current_span(
    name="Smolagent-Trace",
    ) as span:
    
    # Ex√©cutez votre application ici
    response = agent.run("Quelle est la capitale de l'Allemagne ?")
 
    # Passez des attributs suppl√©mentaires au span
    span.update_trace(
        input="Quelle est la capitale de l'Allemagne ?",
        output=response,
        user_id="smolagent-user-123",
        session_id="smolagent-session-123456789",
        tags=["question-ville", "test-agents"],
        metadata={"email": "user@langfuse.com"},
        )
 
# Flusher les √©v√©nements dans les applications de courte dur√©e
langfuse.flush()
```

![Enhancing agent runs with additional metrics](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/bonus-unit2/smolagents-attributes.png)

#### 4. #### 4. Retour utilisateur

Si votre agent est int√©gr√© dans une interface utilisateur, vous pouvez enregistrer les r√©actions directes de l'utilisateur (comme un pouce lev√© ou baiss√© dans une interface de discussion). Vous trouverez ci-dessous un exemple utilisant [Gradio](https://gradio.app/) pour int√©grer un chat avec un m√©canisme de retour d'information simple.

Dans l'extrait de code ci-dessous, lorsqu'un utilisateur envoie un message de chat, nous capturons la trace dans Langfuse. Si l'utilisateur aime ou n'aime pas la derni√®re r√©ponse, nous attribuons un score √† la trace.

```python
import gradio as gr
from smolagents import (CodeAgent, InferenceClientModel)
from langfuse import get_client

langfuse = get_client()

model = InferenceClientModel()
agent = CodeAgent(tools=[], model=model, add_base_tools=True)

trace_id = None

def respond(prompt, history):
    with langfuse.start_as_current_span(
        name="Smolagent-Trace"):
        
        # Ex√©cutez votre application ici
        output = agent.run(prompt)

        global trace_id
        trace_id = langfuse.get_current_trace_id()

    history.append({"role": "assistant", "content": str(output)})
    return history

def handle_like(data: gr.LikeData):
    # Pour la d√©monstration, nous mappons les retours utilisateur √† 1 (j'aime) ou 0 (je n'aime pas)
    if data.liked:
        langfuse.create_score(
            value=1,
            name="user-feedback",
            trace_id=trace_id
        )
    else:
        langfuse.create_score(
            value=0,
            name="user-feedback",
            trace_id=trace_id
        )

with gr.Blocks() as demo:
    chatbot = gr.Chatbot(label="Chat", type="messages")
    prompt_box = gr.Textbox(placeholder="Tapez votre message...", label="Votre message")

    # Lorsque l'utilisateur appuie sur "Enter", nous ex√©cutons 'respond'
    prompt_box.submit(
        fn=respond,
        inputs=[prompt_box, chatbot],
        outputs=chatbot
    )

    # Lorsque l'utilisateur clique sur le bouton "J'aime" d'un message, nous ex√©cutons 'handle_like'
    chatbot.like(handle_like, None, None)

demo.launch()
```

Les retours des utilisateurs sont ensuite saisis dans votre outil d'observabilit√© :

![User feedback is being captured in Langfuse](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/bonus-unit2/user-feedback-gradio.png)

#### 5. LLM-as-a-Judge

*LLM-as-a-Judge* est une autre fa√ßon d'√©valuer automatiquement les r√©sultats de votre agent. Vous pouvez configurer l'appel d'un autre LLM pour √©valuer l'exactitude, la toxicit√©, le style ou tout autre crit√®re qui vous int√©resse.

**Fonctionnement** :
1. Vous d√©finissez un **Mod√®le d'√©valuation**, par exemple, ¬´ V√©rifier si le texte est toxique ¬ª.
2. Chaque fois que votre agent g√©n√®re un r√©sultat, vous transmettez ce r√©sultat √† votre LLM juge avec le gabarit.
3. Le LLM juge r√©pond avec un score ou une √©tiquette que vous enregistrez dans votre outil d'observabilit√©.

Exemple de Langfuse :

![LLM-as-a-Judge Evaluation Template](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/bonus-unit2/evaluator-template.png)
![LLM-as-a-Judge Evaluator](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/bonus-unit2/evaluator.png)

```python
# Exemple : V√©rifier si la sortie de l'agent est toxique ou non
from smolagents import (CodeAgent, DuckDuckGoSearchTool, InferenceClientModel)

search_tool = DuckDuckGoSearchTool()
agent = CodeAgent(tools=[search_tool], model=InferenceClientModel())

agent.run("Manger des carottes peut-il am√©liorer votre vision ?")
```

Vous pouvez voir que la r√©ponse de cet exemple est jug√©e ¬´ non toxique ¬ª.

![LLM-as-a-Judge Evaluation Score](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/bonus-unit2/llm-as-a-judge-score.png)

#### 6. Aper√ßu des m√©triques d'observabilit√©

Toutes ces m√©triques peuvent √™tre visualis√©es ensemble dans des tableaux de bord. Cela vous permet de voir rapidement les performances de votre agent sur plusieurs sessions et vous aide √† suivre les mesures de qualit√© au fil du temps.

![Observability metrics overview](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/bonus-unit2/langfuse-dashboard.png)

## √âvaluation hors ligne

L'√©valuation en ligne est essentielle pour obtenir un retour d'information en temps r√©el, mais vous avez √©galement besoin d'une **√©valuation hors ligne**, c'est-√†-dire de v√©rifications syst√©matiques avant ou pendant le d√©veloppement. Cela permet de maintenir la qualit√© et la fiabilit√© avant de mettre les changements en production.

### √âvaluation d'un jeu de donn√©es

Lors d'une √©valuation hors ligne, vous devez g√©n√©ralement
1. Disposer d'un jeu de donn√©es de r√©f√©rence (avec des paires de *prompts* et de r√©sultats attendus)
2. Ex√©cuter votre agent sur ce jeu de donn√©es
3. Comparer les r√©sultats aux r√©sultats attendus ou utiliser un m√©canisme de notation suppl√©mentaire.

Ci-dessous, nous d√©montrons cette approche avec le jeu de donn√©es [GSM8K](https://huggingface.co/datasets/gsm8k), qui contient des questions et des solutions math√©matiques.

```python
import pandas as pd
from datasets import load_dataset

# R√©cup√©rer GSM8K sur Hugging Face
dataset = load_dataset("openai/gsm8k", 'main', split='train')
df = pd.DataFrame(dataset)
print("Premi√®res lignes du jeu de donn√©es GSM8K :")
print(df.head())
```

Ensuite, nous cr√©ons un jeu de donn√©es dans Langfuse pour suivre les ex√©cutions. Nous ajoutons ensuite chaque √©l√©ment du jeu de donn√©es au syst√®me.  
(Si vous n'utilisez pas Langfuse, vous pouvez simplement les stocker dans votre propre base de donn√©es ou dans un fichier local √† des fins d'analyse).

```python
from langfuse import get_client
langfuse = get_client()

langfuse_dataset_name = "gsm8k_dataset_huggingface"

# Cr√©er un jeu de donn√©es dans Langfuse
langfuse.create_dataset(
    name=langfuse_dataset_name,
    description="Jeu de donn√©es de r√©f√©rence GSM8K t√©l√©charg√© depuis Huggingface",
    metadata={
        "date": "2025-03-10", 
        "type": "benchmark"
    }
)
```

```python
for idx, row in df.iterrows():
    langfuse.create_dataset_item(
        dataset_name=langfuse_dataset_name,
        input={"text": row["question"]},
        expected_output={"text": row["answer"]},
        metadata={"source_index": idx}
    )
    if idx >= 9: # T√©l√©charger seulement les 10 premiers √©l√©ments pour la d√©monstration
        break
```

![Dataset items in Langfuse](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/bonus-unit2/example-dataset.png)

#### Ex√©cution de l'agent sur le jeu de donn√©es

Nous d√©finissons une fonction d'aide `run_smolagent()` qui :
1. D√©marre un span Langfuse
2. Ex√©cute notre agent sur le *prompt*
3. Enregistre l'ID de la trace dans Langfuse

Ensuite, nous parcourons en boucle chaque √©l√©ment de l'ensemble de donn√©es, nous ex√©cutons l'agent et nous lions la trace √† l'√©l√©ment de l'ensemble de donn√©es. Nous pouvons √©galement joindre une note d'√©valuation rapide si vous le souhaitez.

```python
from opentelemetry.trace import format_trace_id
from smolagents import (CodeAgent, InferenceClientModel, LiteLLMModel)
from langfuse import get_client
 
langfuse = get_client()

# Exemple : utiliser InferenceClientModel ou LiteLLMModel pour acc√©der aux mod√®les openai, anthropic, gemini, etc. :
model = InferenceClientModel()

agent = CodeAgent(
    tools=[],
    model=model,
    add_base_tools=True
)

dataset_name = "gsm8k_dataset_huggingface"
current_run_name = "smolagent-notebook-run-01" # Identifie cette ex√©cution d'√©valuation sp√©cifique
 
# Supposons que 'run_smolagent' est votre fonction d'application instrument√©e
def run_smolagent(question):
    with langfuse.start_as_current_generation(name="qna-llm-call") as generation:
        # Simuler un appel LLM
        result = agent.run(question)
 
        # Mettre √† jour la trace avec l'entr√©e et la sortie
        generation.update_trace(
            input= question,
            output=result,
        )
 
        return result
 
dataset = langfuse.get_dataset(name=dataset_name) # R√©cup√©rer votre jeu de donn√©es pr√©-peupl√©
 
for item in dataset.items:
 
    # Utiliser le gestionnaire de contexte item.run()
    with item.run(
        run_name=current_run_name,
        run_metadata={"model_provider": "Hugging Face", "temperature_setting": 0.7},
        run_description="Ex√©cution d'√©valuation pour le jeu de donn√©es GSM8K"
    ) as root_span: # root_span est le span racine de la nouvelle trace pour cet √©l√©ment et ex√©cution.
        # Toutes les op√©rations langfuse suivantes dans ce bloc font partie de cette trace.
 
        # Appelez votre logique d'application
        generated_answer = run_smolagent(question=item.input["text"])
 
        print(item.input)
```

Vous pouvez r√©p√©ter ce processus avec diff√©rents :
- Mod√®les (OpenAI GPT, LLM local, etc.)
- Outils (recherche ou pas recherche)
- Prompts (diff√©rents messages du syst√®me)

Ensuite, comparez-les c√¥te √† c√¥te dans votre outil d'observabilit√© :

![Dataset run overview](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/bonus-unit2/dataset_runs.png)
![Dataset run comparison](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/bonus-unit2/dataset-run-comparison.png)

## R√©flexions finales

Dans ce *notebook*, nous avons vu comment :
1. **Mettre en place l'observabilit√©** en utilisant les exportateurs smolagents + OpenTelemetry
2. **V√©rifier l'instrumentation** en lan√ßant un agent simple
3. **Capturez des m√©triques d√©taill√©es** (co√ªt, latence, etc.) √† l'aide d'outils d'observabilit√©
4. **Recueillir les commentaires des utilisateurs** via une interface Gradio
5. **Utiliser un LLM-as-a-Judge** pour √©valuer automatiquement les r√©sultats
6. **Effectuer une √©valuation hors ligne** avec un jeu de donn√©es de r√©f√©rence

ü§ó Bon codage !