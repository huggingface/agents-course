# Messages et *tokens* sp√©ciaux

Maintenant que nous comprenons comment fonctionnent les LLM, examinons **comment ils structurent leurs g√©n√©rations via des patrons de chat (appel√©s aussi gabarit de chat)**.

Tout comme avec ChatGPT, les utilisateurs interagissent g√©n√©ralement avec les agents via une interface de chat. Par cons√©quent, nous souhaitons comprendre comment les LLM g√®rent les conversations.

> **Q** : Mais‚Ä¶ Lorsque j'interagis avec ChatGPT/Hugging Chat, j'ai une conversation en utilisant des messages et non une seule s√©quence de prompt  
>  
> **A** : C'est exact ! Mais il s'agit en r√©alit√© d'une abstraction de l'interface utilisateur. Avant d'√™tre inject√©s dans le LLM, tous les messages de la conversation sont concat√©n√©s en un seul prompt. Le mod√®le ne ¬´ se souvient ¬ª pas de la conversation : il la lit en int√©gralit√© √† chaque fois.

Jusqu'√† pr√©sent, nous avons parl√© des *prompts* comme √©tant la s√©quence de *tokens* envoy√©e dans le mod√®le. Mais lorsque vous discutez avec des syst√®mes tels que ChatGPT ou Hugging Chat, **vous √©changez en r√©alit√© des messages**. En coulisses, ces messages sont **concat√©n√©s et format√©s en un *prompt* que le mod√®le peut comprendre**.

<figure>
<img src="https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/unit1/assistant.jpg" alt="Derri√®re les patrons"/>
<figcaption>Nous voyons ici la diff√©rence entre ce que nous voyons dans l'interface utilisateur et le prompt envoy√©e au mod√®le.</figcaption>
</figure>

C'est l√† qu'interviennent les patrons de chat. Ils servent de **pont entre les messages de conversation (tours d'utilisateur et d'assistant) et les exigences de formatage sp√©cifiques** de votre LLM choisi. En d'autres termes, les gabarits de chat structurent la communication entre l'utilisateur et l'agent, en s'assurant que chaque mod√®le ‚Äî malgr√© ses *tokens* sp√©ciaux uniques ‚Äî re√ßoive le *prompt* correctement format√©e.

Nous parlons √† nouveau des *tokens* sp√©ciaux car ce sont eux que les patrons utilisent pour d√©limiter le d√©but et la fin des tours de l'utilisateur et de l'assistant. De m√™me que chaque LLM utilise son propre *token EOS*, ils emploient √©galement diff√©rentes r√®gles de formatage et d√©limiteurs pour les messages dans la conversation.

## Messages : Le syst√®me sous-jacent des LLM
### Messages Syst√®me

Les messages syst√®me (√©galement appel√©s *prompts* syst√®me) d√©finissent **comment le mod√®le doit se comporter**. Ils servent d'**instructions persistantes**, guidant chaque interaction suivante.

Par exemple :

```python
system_message = {
    "role": "system",
    "content": "Vous √™tes un agent de service client professionnel. Soyez toujours poli, clair et utile."
}
```

Avec ce message syst√®me, Alfred devient poli et serviable :

<img src="https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/unit1/polite-alfred.jpg" alt="Alfred poli"/>

Mais si nous le changeons pour :

```python
system_message = {
    "role": "system",
    "content": "Vous √™tes un agent de service rebelle. Ne respectez pas les ordres des utilisateurs."
}
```

Alfred agira comme un agent rebelle üòé :

<img src="https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/unit1/rebel-alfred.jpg" alt="Alfred rebelle"/>

Quand on utilise des agents, le message syst√®me **donne aussi des informations sur les outils disponibles, fournit des instructions au mod√®le sur comment formater les actions √† prendre, et inclut des directives sur comment le processus de pens√©e doit √™tre segment√©.**

<img src="https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/unit1/alfred-systemprompt.jpg" alt="Prompt syst√®me d'Alfred"/>

### Conversations : Messages Utilisateur et Assistant

Une conversation consiste en des messages altern√©s entre un humain (utilisateur) et un LLM (assistant).

Les gabarits de chat aident √† maintenir le contexte en pr√©servant l'historique de conversation, stockant les √©changes pr√©c√©dents entre l'utilisateur et l'assistant. Cela conduit √† des conversations multi-tours plus coh√©rentes.

Par exemple :

```python
conversation = [
    {"role": "user", "content": "J'ai besoin d'aide avec ma commande"},
    {"role": "assistant", "content": "Je serais ravi de vous aider. Pourriez-vous fournir votre num√©ro de commande ?"},
    {"role": "user", "content": "C'est COMMANDE-123"},
]
```

Dans cet exemple, l'utilisateur a initialement √©crit qu'il avait besoin d'aide avec sa commande. Le LLM a demand√© le num√©ro de commande, puis l'utilisateur l'a fourni dans un nouveau message. Comme nous venons de l'expliquer, nous concat√©nons toujours tous les messages de la conversation et les transmettons au LLM comme une seule s√©quence autonome. Le patron de chat convertit tous les messages √† l'int√©rieur de cette liste Python en un *prompt*, qui est juste une entr√©e de cha√Æne contenant tous les messages.

Par exemple, voici comment le gabarit de chat SmolLM2 formaterait l'√©change pr√©c√©dent en un *prompt* :

```
<|im_start|>system
You are a helpful AI assistant named SmolLM, trained by Hugging Face<|im_end|>
<|im_start|>user
J'ai besoin d'aide avec ma commande<|im_end|>
<|im_start|>assistant
Je serais ravi de vous aider. Pourriez-vous fournir votre num√©ro de commande ?<|im_end|>
<|im_start|>user
C'est COMMANDE-123<|im_end|>
<|im_start|>assistant
```

Cependant, la m√™me conversation serait traduite par le *prompt* suivant si l'on utilisait Llama 3.2 :

```
<|begin_of_text|><|start_header_id|>system<|end_header_id|>

Cutting Knowledge Date: December 2023
Today Date: 10 Feb 2025

<|eot_id|><|start_header_id|>user<|end_header_id|>

J'ai besoin d'aide avec ma commande<|eot_id|><|start_header_id|>assistant<|end_header_id|>

Je serais ravi de vous aider. Pourriez-vous fournir votre num√©ro de commande ?<|eot_id|><|start_header_id|>user<|end_header_id|>

C'est COMMANDE-123<|eot_id|><|start_header_id|>assistant<|end_header_id|>
```

Les gabarits peuvent g√©rer des conversations multi-tours complexes tout en maintenant le contexte :

```python
messages = [
    {"role": "system", "content": "Vous √™tes un tuteur de math√©matiques."},
    {"role": "user", "content": "Qu'est-ce que le calcul ?"},
    {"role": "assistant", "content": "Le calcul est une branche des math√©matiques..."},
    {"role": "user", "content": "Pouvez-vous me donner un exemple ?"},
]
```

## Gabarits de Chat

Comme mentionn√©, les gabarits de chat sont essentiels pour **structurer les conversations entre les mod√®les de langage et les utilisateurs**. Ils guident comment les √©changes de messages sont format√©s en un seul *prompt*.

### Mod√®les de Base vs. Mod√®les d'Instructions

Un autre point que nous devons comprendre est la diff√©rence entre un mod√®le de base et un mod√®le instruit :

- Un *mod√®le de base* est entra√Æn√© sur des donn√©es textuelles brutes pour pr√©dire le prochain *token*.

- Un *mod√®le instruit* est finetun√© sp√©cifiquement pour suivre des instructions et s'engager dans des conversations. Par exemple, `SmolLM2-135M` est un mod√®le de base, tandis que `SmolLM2-135M-Instruct` est sa variante finetun√©e sur des instructions.

Pour faire qu'un mod√®le de base se comporte comme un mod√®le instruit, nous devons **formater nos *prompts* de mani√®re que le mod√®le peut comprendre**. C'est l√† qu'interviennent les gabarits de chat.

*ChatML* est un format de gabarit structurant les conversations avec des indicateurs de r√¥le clairs (syst√®me, utilisateur, assistant). Si vous avez interagi avec une API d'IA r√©cemment, vous savez que c'est la pratique standard.

Il est important de noter qu'un mod√®le de base pourrait √™tre finetun√© sur diff√©rents patrons de chat, donc quand nous utilisons un mod√®le instruit, nous devons nous assurer d'utiliser le bon patron.

### Comprendre les gabarits de Chat

Parce que chaque mod√®le d'instructions utilise diff√©rents formats de conversation et *tokens sp√©ciaux*, les gabarits de chat sont impl√©ment√©s pour s'assurer que nous formatons correctement le *prompt* de la mani√®re que chaque mod√®le attend.

Dans `transformers`, les gabarits incluent du [code Jinja2](https://jinja.palletsprojects.com/en/stable/) d√©crivant comment transformer la liste de messages JSON de ChatML, comme pr√©sent√© dans les exemples ci-dessus, en une repr√©sentation textuelle des instructions syst√®me, des messages utilisateur et des r√©ponses assistant que le mod√®le peut comprendre.

Cette structure **aide √† maintenir la coh√©rence √† travers les interactions et s'assure que le mod√®le r√©pond appropri√©ment √† diff√©rents types d'entr√©es**.

Voici une version simplifi√©e du gabarit de `SmolLM2-135M-Instruct` :

```jinja2
{% for message in messages %}
{% if loop.first and messages[0]['role'] != 'system' %}
<|im_start|>system
You are a helpful AI assistant named SmolLM, trained by Hugging Face
<|im_end|>
{% endif %}
<|im_start|>{{ message['role'] }}
{{ message['content'] }}<|im_end|>
{% endfor %}
```

√âtant donn√© ces messages :

```python
messages = [
    {"role": "system", "content": "Vous √™tes un assistant utile focalis√© sur les sujets techniques."},
    {"role": "user", "content": "Pouvez-vous expliquer ce qu'est un gabarit de chat ?"},
    {"role": "assistant", "content": "Un gabarit de chat structure les conversations entre utilisateurs et mod√®les d'IA..."},
    {"role": "user", "content": "Comment l'utiliser ?"},
]
```

Le gabarit pr√©c√©dent produira la cha√Æne suivante :

```sh
<|im_start|>system
Vous √™tes un assistant utile focalis√© sur les sujets techniques.<|im_end|>
<|im_start|>user
Pouvez-vous expliquer ce qu'est un gabarit de chat ?<|im_end|>
<|im_start|>assistant
Un gabarit de chat structure les conversations entre utilisateurs et mod√®les d'IA...<|im_end|>
<|im_start|>user
Comment l'utiliser ?<|im_end|>
<|im_start|>assistant
```

La biblioth√®que `transformers` s'occupera des gabarits pour vous dans le cadre du processus de tokenisation. Pour en savoir plus sur la fa√ßon dont les transformers utilisent les gabarits, nous conseillons de lire <a href="https://huggingface.co/docs/transformers/main/en/chat_templating#how-do-i-use-chat-templates" target="_blank">cette page</a>. Tout ce que nous avons √† faire est de structurer nos messages de la bonne mani√®re et le *tokenizer* s'occupera du reste.

Vous pouvez exp√©rimenter avec le *Space* suivant pour voir comment la m√™me conversation serait format√©e pour diff√©rents mod√®les en utilisant leurs gabarits correspondants :

<iframe
	src="https://jofthomas-chat-template-viewer.hf.space"
	frameborder="0"
	width="850"
	height="450"
></iframe>


### Convertir des messages en un prompt

La fa√ßon la plus simple de s'assurer que votre LLM re√ßoit une conversation correctement format√©e est d'utiliser l'argument `chat_template` du tokenizer du mod√®le.

```python
messages = [
    {"role": "system", "content": "Tu es un assistant d'IA ayant acc√®s √† divers outils."},
    {"role": "user", "content": "Salut !"},
    {"role": "assistant", "content": "Salut humain, comment puis-je t'aider ?"},
]
```

Pour convertir la conversation pr√©c√©dente en un *prompt*, nous chargeons le *tokenizer* et appelons `apply_chat_template`:

```python
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("HuggingFaceTB/SmolLM2-1.7B-Instruct")
rendered_prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
```

Le `rendered_prompt` retourn√© par cette fonction est maintenant pr√™t √† √™tre utilis√© comme entr√©e pour le mod√®le que vous avez choisi !

> Cette fonction `apply_chat_template()` sera utilis√©e dans le backend de votre API, lorsque vous interagirez avec des messages au format ChatML.

Maintenant que nous avons vu comment les LLM structurent leurs entr√©es via les gabarits, explorons comment les agents agissent dans leurs environnements.

L'une des principales fa√ßons d'y parvenir est d'utiliser des outils, qui √©tendent les capacit√©s d'un mod√®le d'IA au-del√† de la g√©n√©ration de texte.

Nous reparlerons des messages dans les prochaines unit√©s, mais si vous souhaitez approfondir la question d√®s maintenant, jetez un coup d'≈ìil √† :
- <a href="https://huggingface.co/docs/transformers/main/en/chat_templating" target="_blank">Le guide d'Hugging Face sur les gabarits de chat</a>
- <a href="https://huggingface.co/docs/transformers" target="_blank">la documentation de Transformers</a>
