# Qu'est-ce qu'un LLM ?

<img src="https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/unit1/whiteboard-check-1.jpg" alt="Planification de l'Unit√© 1"/>

Dans la section pr√©c√©dente, nous avons appris que chaque agent a besoin **de se baser sur un mod√®le d'IA** et que les LLM sont le type de mod√®le d'IA le plus courant pour cet usage.

Maintenant, nous allons d√©couvrir ce que sont les LLM et comment ils alimentent les agents.

Cette section offre une explication technique concise sur l'utilisation des LLM. 
Si vous souhaitez approfondir, vous pouvez consulter notre <a href="https://huggingface.co/learn/llm-course/fr/chapter1/1" target="_blank">cours gratuit sur le traitement du langage naturel</a>.

## Qu'est-ce qu'un LLM ?

Un LLM est un type de mod√®le d'IA qui excelle dans **la compr√©hension et la g√©n√©ration du langage humain**. Ils sont entra√Æn√©s sur d'immenses quantit√©s de donn√©es textuelles, ce qui leur permet d'apprendre des motifs, la structure, et m√™me les nuances du langage. Ces mod√®les se composent g√©n√©ralement de plusieurs millions de param√®tres.

La plupart des LLM actuels sont **bas√©s sur l'architecture *Transformer***, une architecture d'apprentissage profond bas√©e sur le m√©canisme d'attention, qui a suscit√© un int√©r√™t consid√©rable depuis la sortie de BERT de Google en 2018.

<figure>
<img src="https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/unit1/transformer.jpg" alt="Transformer"/>
<figcaption>L'architecture originale du Transformer ressemblait √† ceci, avec un encodeur √† gauche et un d√©codeur √† droite.
</figcaption>
</figure>

Il existe 3 types de *transformers* :

1. **Encodeurs**  
   Un *transformer* bas√© sur un encodeur prend en entr√©e un texte (ou d'autres donn√©es) et produit une repr√©sentation dense (aussi appell√©e *embedding*) de ce texte.

   - **Exemple** : BERT de Google
   - **Cas d'utilisation** : Classification de texte, recherche s√©mantique, reconnaissance d'entit√©s nomm√©es
   - **Taille typique** : Des millions de param√®tres

2. **D√©codeurs**  
   Un *transformer* bas√© sur un d√©codeur se concentre **sur la g√©n√©ration de nouveaux *tokens* pour compl√©ter une s√©quence, un *token* √† la fois**.

   - **Exemple** : Llama de Meta 
   - **Cas d'utilisation** : G√©n√©ration de texte, chatbots, g√©n√©ration de code
   - **Taille typique** : Des milliards de param√®tres

3. **Seq2Seq (Encodeur‚ÄìD√©codeur)**  
   Un *transformer* s√©quence-√†-s√©quence _combine_ un encodeur et un d√©codeur. L'encodeur traite d'abord la s√©quence d'entr√©e pour en extraire une repr√©sentation contextuelle, puis le d√©codeur g√©n√®re une s√©quence de sortie.

   - **Exemple** : T5, BART
   - **Cas d'utilisation** : Traduction, r√©sum√©, paraphrase
   - **Taille typique** : Des millions de param√®tres

Bien que les mod√®les de langage de grande taille existent sous diff√©rentes formes, les LLM sont typiquement des mod√®les bas√©s sur le d√©codeur avec des milliards de param√®tres. Voici quelques-uns des LLM les plus connus :

| **Mod√®le**      | **Fournisseur**                     |
|-----------------|-------------------------------------|
| **Deepseek-R1** | DeepSeek                            |
| **GPT4**        | OpenAI                              |
| **Llama**       | Meta (Facebook AI Research)         |
| **SmolLM**      | Hugging Face                        |
| **Gemma**       | Google                              |
| **Mistral**     | Mistral                             |

Le principe fondamental d'un LLM est simple mais tr√®s efficace : **son objectif est de pr√©dire le *token* suivant, √©tant donn√© une s√©quence de *tokens* pr√©c√©dents**. 
Un *token* est l'unit√© d'information avec laquelle travaille un LLM. Vous pouvez consid√©rer un *token* comme s'il s'agissait d'un mot, mais pour des raisons d'efficacit√©, les LLM n'utilisent pas des mots entiers.

Par exemple, alors que l'anglais compte environ 600 000 mots, un LLM peut avoir un vocabulaire d'environ 32 000 *tokens* (comme c'est le cas avec Llama 2). La tokenisation fonctionne souvent sur des unit√©s sous-mot pouvant √™tre combin√©es.

Par exemple, les *tokens* "int√©ress" et "ant" peuvent se combiner pour former "int√©ressant", ou "√©" peut √™tre ajout√© pour former "int√©ress√©".

Vous pouvez exp√©rimenter (en anglais) avec diff√©rents *tokenizers* avec l'application ci-dessous :

<iframe
	src="https://agents-course-the-tokenizer-playground.static.hf.space"
	frameborder="0"
	width="850"
	height="450"
></iframe>

Chaque LLM poss√®de des ***tokens* sp√©ciaux** propres au mod√®le. Le LLM utilise ces *tokens* pour ouvrir et fermer les composants structur√©s de sa g√©n√©ration. Par exemple, pour indiquer le d√©but ou la fin d'une s√©quence, d'un message ou d'une r√©ponse. De plus, les instructions (ou *prompt*) que nous passons au mod√®le sont √©galement structur√©es avec des *tokens* sp√©ciaux. Le plus important d'entre eux est le ***token* de fin de s√©quence** (EOS).

Les formes des tokens *sp√©ciaux* varient grandement selon les fournisseurs de mod√®les.

Le tableau ci-dessous illustre cette diversit√© :

<table>
  <thead>
    <tr>
      <th><strong>Mod√®le</strong></th>
      <th><strong>Fournisseur</strong></th>
      <th><strong>Token EOS</strong></th>
      <th><strong>Fonctionnalit√©</strong></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>GPT4</strong></td>
      <td>OpenAI</td>
      <td><code>&lt;|endoftext|&gt;</code></td>
      <td>Fin du texte du message</td>
    </tr>
    <tr>
      <td><strong>Llama 3</strong></td>
      <td>Meta (Facebook AI Research)</td>
      <td><code>&lt;|eot_id|&gt;</code></td>
      <td>Fin de la s√©quence</td>
    </tr>
    <tr>
      <td><strong>Deepseek-R1</strong></td>
      <td>DeepSeek</td>
      <td><code>&lt;|end_of_sentence|&gt;</code></td>
      <td>Fin du texte du message</td>
    </tr>
    <tr>
      <td><strong>SmolLM2</strong></td>
      <td>Hugging Face</td>
      <td><code>&lt;|im_end|&gt;</code></td>
      <td>Fin de l'instruction ou du message</td>
    </tr>
    <tr>
      <td><strong>Gemma</strong></td>
      <td>Google</td>
      <td><code>&lt;end_of_turn&gt;</code></td>
      <td>Fin du tour de conversation</td>
    </tr>
  </tbody>
</table>

> [!TIP]
> Nous ne vous demandons pas de m√©moriser ces <i>tokens</i> sp√©ciaux mais il est important d'appr√©cier leur diversit√© et le r√¥le qu'ils jouent dans la g√©n√©ration de texte par les LLM. Si vous souhaitez en savoir plus sur les <i>tokens</i> sp√©ciaux, vous pouvez consulter la configuration du mod√®le dans son d√©p√¥t sur le ü§ó Hub. Par exemple, vous pouvez trouver les <i>tokens</i> sp√©ciaux du mod√®le SmolLM2 dans le fichier <a href="https://huggingface.co/HuggingFaceTB/SmolLM2-135M-Instruct/blob/main/tokenizer_config.json">tokenizer_config.json</a>.

## Comprendre la pr√©diction du *token* suivant

On dit que les LLM sont **autoregressifs**, ce qui signifie que **la sortie d'une passe devient l'entr√©e de la suivante**. Cette boucle continue jusqu'√† ce que le mod√®le pr√©dise que le *token* suivant est le *token EOS*, moment o√π le mod√®le peut s'arr√™ter.

<img src="https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/unit1/AutoregressionSchema.gif" alt="Gif visuel de d√©codage autoregressif" width="60%">

En d'autres termes, un LLM d√©codera le texte jusqu'√† atteindre le *token EOS*. Mais que se passe-t-il lors d'une boucle de d√©codage unique ?

Bien que le processus complet puisse √™tre assez technique dans le cadre de l'apprentissage des agents, voici un aper√ßu succinct :

- Une fois le texte d'entr√©e **tokenis√©**, le mod√®le calcule une repr√©sentation de la s√©quence qui capture des informations sur la signification et la position de chaque *token*.
- Cette repr√©sentation est ensuite trait√©e par le mod√®le pour produire des scores classant la probabilit√© que chaque *token* de son vocabulaire soit le suivant dans la s√©quence.

<img src="https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/unit1/DecodingFinal.gif" alt="Gif visuel du d√©codage" width="60%">

En se basant sur ces scores, plusieurs strat√©gies existent pour s√©lectionner les *tokens* afin de compl√©ter la phrase.

- La strat√©gie de d√©codage la plus simple consiste √† toujours choisir le *token* ayant le score maximum.

Vous pouvez interagir vous-m√™me avec le processus de d√©codage de SmolLM2 dans ce *Space* (n'oubliez pas, il d√©code jusqu'√† atteindre un token **EOS** qui est **<|im_end|>** pour ce mod√®le) :

<iframe
	src="https://agents-course-decoding-visualizer.hf.space"
	frameborder="0"
	width="850"
	height="450"
></iframe>

- Mais il existe des strat√©gies de d√©codage plus avanc√©es. Par exemple, le *beam search* (recherche par faisceaux) explore plusieurs s√©quences candidates pour trouver celle ayant le score total maximum, m√™me si certains *tokens* individuels pr√©sentent des scores plus faibles.

<iframe
	src="https://agents-course-beam-search-visualizer.hf.space"
	frameborder="0"
	width="850"
	height="450"
></iframe>

Si vous souhaitez en savoir plus sur le d√©codage, vous pouvez jeter un ≈ìil au [cours de NLP](https://huggingface.co/learn/llm-course/fr/chapter1/1).

## L'attention est tout ce dont vous avez besoin

Un aspect cl√© de l'architecture *transformer* est **l'attention**. Lors de la pr√©diction du mot suivant, tous les mots d'une phrase ne sont pas √©galement importants ; des mots comme ¬´ France ¬ª et ¬´ capitale ¬ª dans la phrase *¬´ La capitale de la France est ‚Ä¶ ¬ª* portent le plus de sens.

<img src="https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/unit1/AttentionSceneFinal.gif" alt="Gif visuel de l'Attention" width="60%">

Ce processus d'identification des mots les plus pertinents pour pr√©dire le *token* suivant s'est r√©v√©l√© incroyablement efficace.

Bien que le principe de base des LLM ‚Äî pr√©dire le *token* suivant ‚Äî soit rest√© constant depuis GPT-2, des avanc√©es significatives ont √©t√© r√©alis√©es lors de la mise √† l'√©chelle des r√©seaux de neurones et dans le fonctionnement du m√©canisme d'attention pour des s√©quences toujours plus longues.

Si vous avez d√©j√† interagi avec des LLM, vous connaissez probablement le terme *longueur de contexte*, qui fait r√©f√©rence au nombre maximum de *tokens* que le LLM peut traiter ainsi qu'√† la _dur√©e d'attention_ maximale dont il dispose.

## L'importance de bien formuler les instructions au LLM

√âtant donn√© que la seule t√¢che d'un LLM est de pr√©dire le *token* suivant en examinant chaque *token* d'entr√©e, et de choisir ceux qui sont ¬´ importants ¬ª, la formulation de votre s√©quence d'entr√©e rev√™t une importance capitale.

La s√©quence d'entr√©e que vous fournissez √† un LLM est appel√©e _prompt_. Une conception minutieuse du *prompt* facilite **l'orientation de la g√©n√©ration du LLM vers la sortie souhait√©e**.

## Comment sont entra√Æn√©s les LLM ?

Les LLM sont entra√Æn√©s sur de grands ensembles de donn√©es textuelles, o√π ils apprennent √† pr√©dire le mot suivant dans une s√©quence gr√¢ce √† un objectif d'apprentissage autosupervis√© ou de mod√©lisation du langage masqu√©.

Gr√¢ce √† cet apprentissage autosupervis√©, le mod√®le apprend la structure de la langue et les **motifs sous-jacents du texte, ce qui lui permet de g√©n√©raliser √† des donn√©es in√©dites**.

Apr√®s ce _pr√©-entra√Ænement_ initial, les LLM peuvent √™tre sp√©cialis√© via un apprentissage supervis√© pour r√©aliser des t√¢ches sp√©cifiques. Par exemple, certains mod√®les sont entra√Æn√©s pour des structures conversationnelles ou l'utilisation d'outils, tandis que d'autres se concentrent sur la classification ou la g√©n√©ration de code.

## Comment puis-je utiliser les LLM ?

Vous avez deux options principales :

1. **Ex√©cuter localement** (si vous disposez du mat√©riel n√©cessaire).

2. **Utiliser un service Cloud/API** (par exemple, via l'API d'inf√©rence sans serveur d'Hugging Face).

Tout au long de ce cours, nous utiliserons principalement des mod√®les via des API du Hub d'Hugging Face. Par la suite, nous explorerons comment ex√©cuter ces mod√®les localement sur votre mat√©riel.

## Comment les LLM sont-ils utilis√©s dans les agents ?

Les LLM sont un composant cl√© des agents, **fournissant la base pour comprendre et g√©n√©rer le langage humain**.

Ils peuvent interpr√©ter les instructions de l'utilisateur, maintenir le contexte dans les conversations, d√©finir un plan et d√©cider quels outils utiliser.

Nous explorerons ces √©tapes en d√©tail dans cette Unit√©, mais pour l'instant, ce qu'il faut retenir, c'est que le LLM est **le cerveau de l'agent**.

---

Cela fait beaucoup d'informations ! Nous avons couvert les bases de ce que sont les LLM, comment ils fonctionnent, et leur r√¥le pour les agents.

Si vous souhaitez plonger encore plus profond√©ment dans le monde fascinant des mod√®les de langage et du traitement du langage naturel, n'h√©sitez pas √† consulter notre <a href="https://huggingface.co/learn/llm-course/fr/chapter1/1" target="_blank">cours gratuit sur le NLP</a>.

Maintenant que nous comprenons le fonctionnement des LLM, il est temps de voir **comment ils structurent leurs g√©n√©rations dans un contexte conversationnel**.

Pour ex√©cuter le <a href="https://huggingface.co/agents-course/notebooks/blob/main/fr/unit1/dummy_agent_library.ipynb" target="_blank"><i>notebook</i></a>, **vous avez besoin d'un *token* d'authentication Hugging Face** que vous pouvez obtenir sur la page <a href="https://hf.co/settings/tokens" target="_blank">https://hf.co/settings/tokens</a>.

Vous devez √©galement demander l'acc√®s aux <a href="https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct" target="_blank">mod√®les Llama 3.2 de Meta</a>.
