# Finetunons un mod√®le pour pouvoir faire de l'appel de fonctions

Nous sommes maintenant pr√™ts √† finetuner notre premier mod√®le pour de l'appel de fonctions üî•.

## Comment entra√Ænons-nous un tel mod√®le ?

> R√©ponse : Nous avons besoin de **donn√©es**

Un processus d'entra√Ænement peut √™tre divis√© en 3 √©tapes :

1. **Le mod√®le est pr√©-entra√Æn√© sur une grande quantit√© de donn√©es**. Le r√©sultat de cette √©tape est un **mod√®le pr√©-entra√Æn√©**. Par exemple, [google/gemma-2-2b](https://huggingface.co/google/gemma-2-2b). C'est un mod√®le de base et il sait seulement comment **pr√©dire le prochain *token* sans fortes capacit√©s de suivi d'instructions**.

2. Pour √™tre utile dans un contexte de conversation, le mod√®le doit ensuite √™tre **finetun√©** pour suivre des instructions. √Ä cette √©tape, il peut √™tre entra√Æn√© par les cr√©ateurs du mod√®le, la communaut√© open-source, vous, ou n'importe qui. Par exemple, [google/gemma-2-2b-it](https://huggingface.co/google/gemma-2-2b-it) est un mod√®le finetun√© pour les instructions par l'√©quipe Google derri√®re le projet Gemma.

3. Le mod√®le peut ensuite √™tre **align√©** selon les pr√©f√©rences du cr√©ateur. Par exemple, un mod√®le conversationnel d'un service client  ne doit jamais √™tre impoli avec l'utilisateur.

Habituellement, un produit complet comme *Gemini* ou *Mistral* **sera pass√© par les 3 √©tapes**, alors que les mod√®les que vous pouvez trouver sur *Hugging Face* ont effectu√© une ou plusieurs √©tapes de cet entra√Ænement.

Dans ce tutoriel, nous allons construire un mod√®le d'appel de fonctions bas√© sur [google/gemma-2-2b-it](https://huggingface.co/google/gemma-2-2b-it). Nous choisissons le mod√®le  [google/gemma-2-2b-it](https://huggingface.co/google/gemma-2-2b-it) au lieu du mod√®le de base [google/gemma-2-2b](https://huggingface.co/google/gemma-2-2b) parce que le mod√®le finetun√© a √©t√© am√©lior√© pour notre cas d'usage.

Partir du mod√®le pr√©-entra√Æn√© **n√©cessiterait plus d'entra√Ænement pour apprendre le suivi d'instructions, le chat ET l'appel de fonctions**.

En partant du mod√®le finetun√© pour les instructions, **nous minimisons la quantit√© d'informations que notre mod√®le doit apprendre**.

## LoRA (Low-Rank Adaptation of Large Language Models)

LoRA est une technique d'entra√Ænement populaire et l√©g√®re qui **r√©duit significativement le nombre de param√®tres √† entra√Æner**.

Elle fonctionne en **ins√©rant un lot d'adaptateurs constitu√©s d'un petit nombre de nouveaux poids,  dans le mod√®le √† entra√Æner**. Cela rend l'entra√Ænement avec LoRA beaucoup plus rapide, √©conome en m√©moire, et produit des poids de mod√®le plus petits (quelques centaines de MB), qui sont plus faciles √† stocker et partager.

<img src="https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/unit1/blog_multi-lora-serving_LoRA.gif" alt="LoRA inference" width="50%"/>

LoRA fonctionne en ajoutant des paires de matrices de d√©composition de rang aux couches d'un *transformer* (typiquement les couches lin√©aires). Durant l'entra√Ænement, nous g√®lons le reste du mod√®le et ne mettons √† jour uniquement les poids de ces adaptateurs ajout√©s.

Ce faisant, le nombre de **param√®tres** que nous devons entra√Æner diminue consid√©rablement car nous devons seulement mettre √† jour les poids des adaptateurs.

Durant l'inf√©rence, l'entr√©e est pass√©e dans les adaptateurs et le mod√®le de base. Ou bien les poids des adaptateurs peuvent √™tre fusionn√©s avec le mod√®le de base, ne r√©sultant en aucune surcharge de latence suppl√©mentaire.

LoRA est particuli√®rement utile pour adapter de **grands** mod√®les de langage √† des t√¢ches ou domaines sp√©cifiques tout en gardant les exigences de ressources g√©rables. Cela aide √† r√©duire la m√©moire **requise** pour entra√Æner un mod√®le.

Si vous voulez en savoir plus sur comment LoRA fonctionne, vous devriez consulter ce [tutoriel](https://huggingface.co/learn/nlp-course/chapter11/4?fw=pt).

## Finetuning d'un mod√®le pour l'appel de fonctions

La suite de cette section se passe dans le *notebook* du tutoriel que vous pouvez acc√©der üëâ [ici](https://huggingface.co/agents-course/notebooks/blob/main/fr/bonus-unit1/bonus-unit1.ipynb).

Ensuite, cliquez sur [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/#fileId=https://huggingface.co/agents-course/notebooks/blob/main/fr/bonus-unit1/bonus-unit1.ipynb) pour pouvoir l'ex√©cuter dans Colab.
