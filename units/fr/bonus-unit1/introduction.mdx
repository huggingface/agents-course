# Introduction

![Bonus Unit 1 Thumbnail](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/bonus-unit1/thumbnail.jpg)

Bienvenue dans cette premi√®re **Unit√© Bonus**, o√π vous apprendrez √† **finetuner un LLM pour de l'appel de fonctions** (*function calling*).

En termes de LLM, l'appel de fonctions devient rapidement une technique *incontournable*. 

L'id√©e est que, plut√¥t que de s'appuyer uniquement sur des approches bas√©es sur des *prompts* comme nous l'avons fait dans l'Unit√© 1, l'appel de fonctions entra√Æne votre mod√®le √† **prendre des actions et interpr√©ter des observations pendant la phase d'entra√Ænement**, rendant votre IA plus robuste.

> **Quand dois-je faire cette Unit√© Bonus ?**
>
> Cette section est **optionnelle** et plus avanc√©e que l'Unit√© 1, donc n'h√©sitez pas √† faire cette unit√© maintenant ou √† la revisiter quand vos connaissances se seront davantage d√©velopp√©es gr√¢ce √† ce cours. 
>  
> Mais ne vous inqui√©tez pas, cette Unit√© Bonus est con√ßue pour avoir toutes les informations dont vous avez besoin, donc nous vous guiderons √† travers chaque concept central du finetuning d'un mod√®le d'appel de fonctions m√™me si vous n'avez pas encore appris le fonctionnement interne de ce type de finetuning.

La meilleure fa√ßon pour vous de pouvoir suivre cette Unit√© Bonus est de :

1. Savoir comment finetuner un mod√®le avec *Transformers*. Si ce n'est pas le cas [consultez cette page](https://huggingface.co/learn/nlp-course/fr/chapter3/1?fw=pt).

2. Savoir comment utiliser `SFTTrainer` de *TRL* pour finetuner un mod√®le. Pour en savoir plus √† ce sujet [consultez cette documentation](https://huggingface.co/learn/nlp-course/en/chapter11/1). 

---

## Ce que vous allez apprendre

1. **L'appel de fonctions** (*Function Calling*)  
   Comment les LLM modernes structurent leurs conversations de mani√®re efficace afin de d√©clencher des **outils**.

2. **LoRA** (*Low-Rank Adaptation*)  
   Une m√©thode de finetuning **l√©g√®re et efficace** qui r√©duit les co√ªts computationnels et de stockage. LoRA rend l'entra√Ænement de gros mod√®les *plus rapide, moins cher et plus facile* √† d√©ployer.

3. **Le cycle R√©flexion ‚Üí Action ‚Üí Observation** dans les mod√®les d'appel de fonctions  
   Une approche simple mais puissante pour structurer comment votre mod√®le d√©cide quand (et comment) appeler des fonctions, suivre les √©tapes interm√©diaires et interpr√©ter les r√©sultats des outils ou APIs externes.

4. **De nouveaux *tokens* sp√©ciaux**  
   Nous introduirons des **marqueurs sp√©ciaux** qui aident le mod√®le √† distinguer entre :
   - Le raisonnement interne "*chain-of-thought*"  
   - Les appels de fonctions sortants  
   - Les r√©ponses provenant d'outils externes

---

√Ä la fin de cette unit√© bonus, vous serez capable de :

- **Comprendre** le fonctionnement interne des APIs quand il s'agit d'outils.  
- **Finetuner** un mod√®le en utilisant la technique LoRA.  
- **Impl√©menter** et **modifier** le cycle R√©flexion ‚Üí Action ‚Üí Observation pour cr√©er des *workflow* d'appel de fonctions robustes et maintenables.  
- **Concevoir et utiliser** des *tokens* sp√©ciaux pour s√©parer de mani√®re transparente le raisonnement interne du mod√®le de ses actions externes.

Et vous **aurez finetun√© votre propre mod√®le pour faire de l'appel de fonctions.** üî•

Plongeons dans **l'appel de fonctions** !