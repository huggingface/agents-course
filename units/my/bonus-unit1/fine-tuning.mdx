# Function-Calling အတွက် သင့် Model ကို Fine-Tune လုပ်ကြစို့

Function-Calling အတွက် ကျွန်တော်တို့ရဲ့ ပထမဆုံး Model ကို Fine-Tune လုပ်ဖို့ အခုဆိုရင် အသင့်ဖြစ်နေပါပြီ 🔥။

## Function-Calling အတွက် ကျွန်တော်တို့ရဲ့ Model ကို ဘယ်လို လေ့ကျင့်ပေးမလဲ။

> အဖြေ: ကျွန်တော်တို့ **ဒေတာ (Data)** လိုအပ်ပါတယ်။

Model တစ်ခုကို လေ့ကျင့်ပေးတဲ့ လုပ်ငန်းစဉ်ကို အဆင့် (၃) ဆင့် ခွဲခြားနိုင်ပါတယ်။

**၁။ ဒေတာ အမြောက်အမြားဖြင့် ကြိုတင်လေ့ကျင့်ခြင်း (Pre-training):**

Model ကို ဒေတာ အမြောက်အမြားဖြင့် ကြိုတင်လေ့ကျင့်ပေးပါတယ်။ ဒီအဆင့်ရဲ့ ရလဒ်ကတော့ **Pre-trained Model (ကြိုတင်လေ့ကျင့်ပြီးသား မော်ဒယ်)** ပဲ ဖြစ်ပါတယ်။ ဥပမာအားဖြင့်၊ [google/gemma-2-2b](https://huggingface.co/google/gemma-2-2b) ကို ကြည့်ပါ။ ဒါဟာ အခြေခံ Model တစ်ခုဖြစ်ပြီး၊ **ခိုင်မာတဲ့ ညွှန်ကြားချက် လိုက်နာနိုင်စွမ်း မရှိဘဲ နောက်ထပ် Token ကို ခန့်မှန်းဖို့** လောက်သာ သိရှိပါသေးတယ်။

**၂။ ညွှန်ကြားချက် လိုက်နာရန် Fine-tuning လုပ်ခြင်း (Instruction-Tuning):**

Chatting (စကားပြောဆိုမှု) ပတ်ဝန်းကျင်မှာ အသုံးဝင်ဖို့အတွက် Model ကို ညွှန်ကြားချက်များ လိုက်နာနိုင်အောင် **Fine-tune (အသေးစိတ် ချိန်ညှိ)** လုပ်ဖို့ လိုအပ်ပါတယ်။ ဒီအဆင့်ကို Model ဖန်တီးသူများ၊ Open-Source Community သို့မဟုတ် သင်ကိုယ်တိုင် လုပ်ဆောင်နိုင်ပါတယ်။ ဥပမာအားဖြင့်၊ [google/gemma-2-2b-it](https://huggingface.co/google/gemma-2-2b-it) ဟာ Gemma Project ကို ဖန်တီးတဲ့ Google Team က ညွှန်ကြားချက်ဖြင့် Fine-tune လုပ်ထားတဲ့ Model တစ်ခု ဖြစ်ပါတယ်။

**၃။ ဖန်တီးသူ၏ ဦးစားပေးမှုများနှင့် ကိုက်ညီအောင် ချိန်ညှိခြင်း (Alignment):**

ဒီအဆင့်မှာတော့ Model ကို ဖန်တီးသူရဲ့ စိတ်ကြိုက် ဦးစားပေးမှုများနဲ့ ကိုက်ညီအောင် **Aligned (ချိန်ညှိ)** လုပ်ပါတယ်။ ဥပမာအားဖြင့်၊ Customer Service Chat Model တစ်ခုဟာ Customer တွေကို ဘယ်တော့မှ ရိုင်းရိုင်းစိုင်းစိုင်း မပြောရဘူးဆိုတဲ့ စည်းမျဉ်းမျိုးကို ထည့်သွင်း ချိန်ညှိတာမျိုး ဖြစ်ပါတယ်။

ပုံမှန်အားဖြင့် Gemini သို့မဟုတ် Mistral ကဲ့သို့သော ပြည့်စုံတဲ့ Product တွေဟာ **ဒီအဆင့် (၃) ဆင့်လုံးကို ဖြတ်သန်းရပါတယ်**။ Hugging Face မှာ သင်တွေ့နိုင်တဲ့ Model တွေကတော့ ဒီအဆင့်တွေထဲက တစ်ဆင့် သို့မဟုတ် တစ်ဆင့်ထက်ပိုပြီး ပြီးမြောက်ထားတဲ့ Model တွေ ဖြစ်ပါတယ်။

ဒီ Tutorial မှာတော့ ကျွန်တော်တို့ဟာ [google/gemma-2-2b-it](https://huggingface.co/google/gemma-2-2b-it) ကို အခြေခံပြီး Function-Calling Model တစ်ခုကို တည်ဆောက်သွားမှာပါ။ ကျွန်တော်တို့ဟာ အခြေခံ Model ဖြစ်တဲ့ [google/gemma-2-2b](https://huggingface.co/google/gemma-2-2b) အစား Fine-tune လုပ်ပြီးသား Model ဖြစ်တဲ့ [google/gemma-2-2b-it](https://huggingface.co/google/gemma-2-2b-it) ကို ရွေးချယ်ရခြင်းဟာ ဒီ Model က ကျွန်တော်တို့ရဲ့ အသုံးပြုမှုအတွက် ပိုမိုကောင်းမွန်အောင် ပြုပြင်ထားပြီးသား ဖြစ်လို့ပါ။

Pre-trained Model ကနေ စတင်မယ်ဆိုရင် **ညွှန်ကြားချက် လိုက်နာခြင်း၊ Chatting လုပ်ဆောင်နိုင်ခြင်းနဲ့ Function-Calling လုပ်ဆောင်နိုင်ခြင်း** တွေကို သင်ယူဖို့အတွက် **ပိုမိုများပြားတဲ့ လေ့ကျင့်မှုတွေ** လိုအပ်ပါလိမ့်မယ်။

Instruction-tuned Model ကနေ စတင်ခြင်းအားဖြင့် **ကျွန်တော်တို့ရဲ့ Model သင်ယူဖို့ လိုအပ်တဲ့ အချက်အလက် ပမာဏကို လျှော့ချနိုင်ပါတယ်**။

## LoRA (Low-Rank Adaptation of Large Language Models)

LoRA ဟာ လူကြိုက်များပြီး အသုံးပြုရလွယ်ကူတဲ့ လေ့ကျင့်ရေး နည်းပညာတစ်ခုဖြစ်ပြီး၊ **လေ့ကျင့်ရမယ့် Parameter အရေအတွက်ကို သိသိသာသာ လျှော့ချပေးပါတယ်**။

ဒီနည်းလမ်းက Model ထဲကို **Adapter အဖြစ် အလေးချိန်အသစ် အနည်းငယ်ကို ထည့်သွင်းပြီး လေ့ကျင့်ပေးခြင်း** ဖြင့် အလုပ်လုပ်ပါတယ်။ ဒါကြောင့် LoRA နဲ့ လေ့ကျင့်ပေးတာဟာ ပိုမိုမြန်ဆန်ပြီး၊ Memory သုံးစွဲမှု သက်သာစေကာ၊ Model ရဲ့ အလေးချိန် (Weights) တွေဟာလည်း သေးငယ်ပါတယ် (MB အနည်းငယ်သာ)။ ဒါကြောင့် သိမ်းဆည်းဖို့နဲ့ မျှဝေဖို့ ပိုမိုလွယ်ကူပါတယ်။

<img src="https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/unit1/blog_multi-lora-serving_LoRA.gif" alt="LoRA inference" width="50%"/>

LoRA ဟာ Transformer Layers တွေထဲကို Rank Decomposition Matrices များကို ထည့်သွင်းခြင်းဖြင့် အလုပ်လုပ်ပါတယ်။ အများအားဖြင့် Linear Layers တွေကို အာရုံစိုက်ပါတယ်။ လေ့ကျင့်နေစဉ်အတွင်းမှာ Model ရဲ့ ကျန်ရှိတဲ့ အစိတ်အပိုင်းတွေကို **"Freeze" (အေးခဲ)** ထားပြီး၊ အသစ်ထည့်သွင်းထားတဲ့ Adapter ရဲ့ အလေးချိန်တွေကိုသာ Update လုပ်သွားမှာ ဖြစ်ပါတယ်။

ဒီလိုလုပ်ဆောင်ခြင်းအားဖြင့် ကျွန်တော်တို့ လေ့ကျင့်ဖို့ လိုအပ်တဲ့ **Parameters အရေအတွက်** ဟာ သိသိသာသာ လျော့ကျသွားပါတယ်။ (Adapter ရဲ့ အလေးချိန်တွေကိုသာ Update လုပ်ဖို့ လိုအပ်လို့ပါ)။

**အချက်အလက် ထပ်မံဖြည့်စွက်ချက်:** LoRA ကို အသုံးပြုခြင်းကြောင့် GPU Memory သုံးစွဲမှုဟာ ပုံမှန် Fine-tuning လုပ်တာထက် **၁၀ ဆခန့်အထိ** သက်သာနိုင်ပါတယ်။ ဒါကြောင့် သာမန် ကွန်ပျူတာများမှာတောင် LLM များကို စမ်းသပ် Fine-tuning လုပ်နိုင်တဲ့ အခွင့်အရေးကို ရရှိစေပါတယ်။

Inference (ရလဒ်ထုတ်ယူခြင်း) လုပ်တဲ့အခါမှာတော့ Input ကို Adapter နဲ့ Base Model နှစ်ခုလုံးဆီ ပေးပို့ပါတယ်။ ဒါမှမဟုတ် ဒီ Adapter Weights တွေကို Base Model နဲ့ ပေါင်းစပ်လိုက်နိုင်တဲ့အတွက် နောက်ထပ် Latency (နှောင့်နှေးမှု) လုံးဝ မရှိဘဲ ရလဒ်ကို ထုတ်ပေးနိုင်ပါတယ်။

LoRA ဟာ **ကြီးမားတဲ့** Language Model တွေကို သီးခြား Tasks တွေ သို့မဟုတ် Domains တွေနဲ့ လိုက်လျောညီထွေဖြစ်အောင် ပြုပြင်ရာမှာ အထူးအသုံးဝင်ပြီး၊ Resource လိုအပ်ချက်တွေကို ထိန်းချုပ်နိုင်စေပါတယ်။ ဒါက Model တစ်ခုကို လေ့ကျင့်ဖို့ **လိုအပ်တဲ့ Memory** ကို လျှော့ချပေးပါတယ်။

LoRA ဘယ်လိုအလုပ်လုပ်လဲဆိုတာကို ပိုမိုနားလည်ချင်တယ်ဆိုရင်၊ [ဒီ Tutorial ကို လေ့လာကြည့်ရှုနိုင်ပါတယ်](https://huggingface.co/learn/nlp-course/chapter11/4?fw=pt) ။

## Function-Calling အတွက် Model ကို Fine-Tuning လုပ်ခြင်း

Tutorial Notebook ကို 👉 [ဒီနေရာမှာ ဝင်ရောက်ကြည့်ရှုနိုင်ပါတယ်](https://huggingface.co/agents-course/notebooks/blob/main/bonus-unit1/bonus-unit1.ipynb) ။

ပြီးရင်တော့ Colab Notebook မှာ Run နိုင်ဖို့အတွက် [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/#fileId=https://huggingface.co/agents-course/notebooks/blob/main/bonus-unit1/bonus-unit1.ipynb) ကို Click နှိပ်လိုက်ပါ ။