# LLM တွေဆိုတာက ဘာလဲ? (What are LLMs?)

<img src="https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/unit1/whiteboard-check-1.jpg" alt="Unit 1 planning"/>

အရင်အပိုင်းမှာ Agent တစ်ခုစီတိုင်းမှာ **AI Model တစ်ခုက အဓိက အစိတ်အပိုင်းအဖြစ် ပါဝင်ရမယ်** ဆိုတာကို ကျွန်တော်တို့ လေ့လာခဲ့ပြီးပါပြီ။ ဒီရည်ရွယ်ချက်အတွက် LLM (Large Language Model) တွေဟာ အသုံးအများဆုံး AI Model အမျိုးအစား ဖြစ်ပါတယ်။

အခု ဒီအပိုင်းမှာတော့ LLM တွေဆိုတာ ဘာလဲ၊ ပြီးတော့ သူတို့က Agent တွေကို ဘယ်လို စွမ်းအားဖြည့်ပေးသလဲဆိုတာကို လေ့လာသွားပါမယ်။

ဒီအပိုင်းက LLM တွေရဲ့ နည်းပညာဆိုင်ရာ ရှင်းလင်းချက်ကို အကျဉ်းချုပ် ဖော်ပြပေးထားပါတယ်။ ပိုပြီး နက်နက်နဲနဲ လေ့လာချင်တယ်ဆိုရင်တော့ ကျွန်တော်တို့ရဲ့ [အခမဲ့ သဘာဝ ဘာသာစကား စီမံဆောင်ရွက်ခြင်း (Natural Language Processing) Course](https://huggingface.co/learn/nlp-course/chapter1/1) ကို ကြည့်ရှုနိုင်ပါတယ်။

## Large Language Model ဆိုတာ ဘာလဲ?

LLM ဆိုတာဟာ **လူသားတို့ရဲ့ ဘာသာစကားကို နားလည်ပြီး ဖန်တီးထုတ်လုပ်ရာမှာ ထူးချွန်တဲ့** AI Model အမျိုးအစားတစ်ခု ဖြစ်ပါတယ်။ သူတို့ကို စာသား ဒေတာအမြောက်အမြားနဲ့ လေ့ကျင့်ပေးထားတဲ့အတွက် ဘာသာစကားရဲ့ ပုံစံများ၊ ဖွဲ့စည်းပုံနဲ့ အသေးစိတ် အနုစိတ် အဓိပ္ပာယ်တွေကိုပါ သင်ယူနိုင်ပါတယ်။ ဒီ Model တွေဟာ များသောအားဖြင့် **သန်းပေါင်းများစွာသော ပါရာမီတာများ (Parameters)** နဲ့ ဖွဲ့စည်းထားပါတယ်။

ယနေ့ခေတ် LLM အများစုဟာ **Transformer Architecture** ပေါ်မှာ တည်ဆောက်ထားတာ ဖြစ်ပါတယ်။ Transformer ဆိုတာကတော့ ၂၀၁၈ ခုနှစ်မှာ Google က BERT ကို ထုတ်ပြန်ပြီးနောက်ပိုင်း အလွန်စိတ်ဝင်စားမှု ရရှိလာခဲ့တဲ့ "Attention" Algorithm ကို အခြေခံထားတဲ့ Deep Learning Architecture တစ်ခုပဲ ဖြစ်ပါတယ်။

<figure>
<img src="https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/unit1/transformer.jpg" alt="Transformer"/>
<figcaption>မူရင်း Transformer Architecture မှာ ဘယ်ဘက်မှာ Encoder နဲ့ ညာဘက်မှာ Decoder ပါဝင်ပါတယ်။
</figcaption>
</figure>

Transformer တွေမှာ အဓိကအားဖြင့် (၃) မျိုး ရှိပါတယ်။

**၁။ Encoder များ (Encoders)**
Encoder-based Transformer တစ်ခုဟာ စာသား (သို့မဟုတ် အခြားဒေတာ) ကို ထည့်သွင်းမှုအဖြစ် ယူပြီး၊ ထိုစာသားရဲ့ **သိပ်သည်းသော ကိုယ်စားပြုမှု (Dense Representation)** သို့မဟုတ် **Embeddings** ကို ထုတ်ပေးပါတယ်။

*   **ဥပမာ:** Google မှ BERT
*   **အသုံးချ ကိစ္စရပ်များ:** စာသား အမျိုးအစားခွဲခြားခြင်း (Text Classification)၊ အဓိပ္ပာယ်ရှာဖွေခြင်း (Semantic Search)၊ အမည်ပါ အရာဝတ္ထု ခွဲခြားသိရှိခြင်း (Named Entity Recognition)
*   **ပုံမှန် အရွယ်အစား:** သန်းပေါင်းများစွာသော ပါရာမီတာများ

**၂။ Decoder များ (Decoders)**
Decoder-based Transformer တစ်ခုဟာ **တစ်ကြိမ်လျှင် Token တစ်ခုနှုန်းဖြင့်** စာသား အစီအစဉ်တစ်ခုကို ပြီးမြောက်စေရန် **Token အသစ်များ ထုတ်ပေးခြင်း** ကို အဓိကထား လုပ်ဆောင်ပါတယ်။

*   **ဥပမာ:** Meta မှ Llama
*   **အသုံးချ ကိစ္စရပ်များ:** စာသား ဖန်တီးထုတ်လုပ်ခြင်း၊ Chatbot များ၊ Code ဖန်တီးခြင်း
*   **ပုံမှန် အရွယ်အစား:** ဘီလီယံပေါင်းများစွာသော ပါရာမီတာများ (US စနစ်အရ $10^9$)

**၃။ Seq2Seq (Encoder–Decoder)**
Sequence-to-sequence Transformer တစ်ခုဟာ Encoder နဲ့ Decoder ကို **ပေါင်းစပ်ထားတာ** ဖြစ်ပါတယ်။ Encoder က ထည့်သွင်းစာသားကို Context ကိုယ်စားပြုမှုအဖြစ် စီမံဆောင်ရွက်ပြီးနောက်၊ Decoder က ထွက်ရှိလာမယ့် စာသား အစီအစဉ်ကို ဖန်တီးထုတ်ပေးပါတယ်။

*   **ဥပမာ:** T5, BART
*   **အသုံးချ ကိစ္စရပ်များ:** ဘာသာပြန်ဆိုခြင်း၊ အနှစ်ချုပ်ခြင်း၊ စာသားကို ပုံစံပြောင်းလဲခြင်း (Paraphrasing)
*   **ပုံမှန် အရွယ်အစား:** သန်းပေါင်းများစွာသော ပါရာမီတာများ

Large Language Model တွေဟာ ပုံစံအမျိုးမျိုးနဲ့ လာနိုင်ပေမယ့်၊ LLM တွေဟာ များသောအားဖြင့် **ဘီလီယံပေါင်းများစွာသော ပါရာမီတာများပါဝင်တဲ့ Decoder-based Model များ** ဖြစ်ကြပါတယ်။ လူသိများတဲ့ LLM အချို့ကို အောက်ပါဇယားမှာ ဖော်ပြထားပါတယ်။

| **Model** | **Provider** |
| :--- | :--- |
| **Deepseek-R1** | DeepSeek |
| **GPT4** | OpenAI |
| **Llama 3** | Meta (Facebook AI Research) |
| **SmolLM2** | Hugging Face |
| **Gemma** | Google |
| **Mistral** | Mistral |

LLM တစ်ခုရဲ့ အခြေခံ သဘောတရားက ရိုးရှင်းပေမယ့် အလွန်ထိရောက်ပါတယ်။ ၎င်းရဲ့ ရည်ရွယ်ချက်က **အရင် Token များရဲ့ အစီအစဉ်ကို ကြည့်ပြီး နောက်ထပ် Token ကို ခန့်မှန်းဖို့** ပဲ ဖြစ်ပါတယ်။ **"Token"** ဆိုတာက LLM အလုပ်လုပ်တဲ့ အချက်အလက်ယူနစ်ပဲ ဖြစ်ပါတယ်။ Token ကို "စကားလုံး" လို့ ယူဆနိုင်ပေမယ့်၊ စွမ်းဆောင်ရည် ပိုမိုကောင်းမွန်စေဖို့အတွက် LLM တွေက စကားလုံးအပြည့်အစုံကို အသုံးမပြုပါဘူး။

ဥပမာအားဖြင့်၊ အင်္ဂလိပ်ဘာသာမှာ စကားလုံး ၆၀၀,၀၀၀ ခန့် ရှိတယ်လို့ ခန့်မှန်းရပေမယ့်၊ LLM တစ်ခုမှာ Token ၃၂,၀၀၀ ခန့်သာ ပါဝင်တဲ့ ဝေါဟာရစာရင်း (Vocabulary) ရှိနိုင်ပါတယ်။ (Llama 2 မှာ ဒီလိုပါပဲ)။ Tokenization ဟာ များသောအားဖြင့် စကားလုံးရဲ့ အစိတ်အပိုင်း (Sub-word units) တွေပေါ်မှာ အလုပ်လုပ်ပြီး၊ ၎င်းတို့ကို ပေါင်းစပ်နိုင်ပါတယ်။

ဥပမာအနေနဲ့၊ "interest" နဲ့ "ing" ဆိုတဲ့ Token တွေကို ပေါင်းစပ်ပြီး "interesting" ဆိုတဲ့ စကားလုံးကို ဘယ်လို ဖန်တီးနိုင်သလဲဆိုတာကို စဉ်းစားကြည့်ပါ။

အောက်ပါ Interactive Playground မှာ မတူညီတဲ့ Tokenizer တွေကို စမ်းသပ်ကြည့်နိုင်ပါတယ်။

<iframe
	src="https://agents-course-the-tokenizer-playground.static.hf.space"
	frameborder="0"
	width="850"
	height="450"
></iframe>

LLM တစ်ခုစီတိုင်းမှာ Model အတွက် သီးသန့်ဖြစ်တဲ့ **Special Tokens (အထူး Token များ)** အချို့ ရှိပါတယ်။ LLM ဟာ ၎င်းတို့ရဲ့ ထုတ်လုပ်မှုမှာ စနစ်တကျ ဖွဲ့စည်းထားတဲ့ အစိတ်အပိုင်းတွေကို စတင်ဖို့နဲ့ အဆုံးသတ်ဖို့အတွက် ဒီ Token တွေကို အသုံးပြုပါတယ်။ ဥပမာအားဖြင့်၊ စာသား အစီအစဉ်တစ်ခု၊ Message တစ်ခု သို့မဟုတ် တုံ့ပြန်မှုတစ်ခုရဲ့ အစ သို့မဟုတ် အဆုံးကို ဖော်ပြဖို့အတွက် ဖြစ်ပါတယ်။ ဒါ့အပြင်၊ ကျွန်တော်တို့ Model ကို ပေးပို့တဲ့ Input Prompts တွေကိုလည်း Special Tokens တွေနဲ့ ဖွဲ့စည်းထားပါတယ်။ အဲဒီထဲမှာ အရေးအကြီးဆုံးကတော့ **End of Sequence Token (EOS)** ပဲ ဖြစ်ပါတယ်။

Special Tokens တွေရဲ့ ပုံစံတွေဟာ Model Provider တွေအလိုက် အလွန်ကွဲပြားပါတယ်။

အောက်ပါဇယားက Special Tokens တွေရဲ့ ကွဲပြားမှုကို ဖော်ပြထားပါတယ်။

| **Model** | **Provider** | **EOS Token** | **လုပ်ဆောင်နိုင်စွမ်း** |
| :--- | :--- | :--- | :--- |
| **GPT4** | OpenAI | `&lt;|endoftext|&gt;` | Message စာသား၏ အဆုံး |
| **Llama 3** | Meta (Facebook AI Research) | `&lt;|eot_id|&gt;` | Sequence ၏ အဆုံး |
| **Deepseek-R1** | DeepSeek | `&lt;|end_of_sentence|&gt;` | Message စာသား၏ အဆုံး |
| **SmolLM2** | Hugging Face | `&lt;|im_end|&gt;` | ညွှန်ကြားချက် သို့မဟုတ် Message ၏ အဆုံး |
| **Gemma** | Google | `&lt;end_of_turn&gt;` | စကားပြောဆိုမှု အလှည့်၏ အဆုံး |

> [!TIP]
> ဒီ Special Tokens တွေကို သင်တို့ မှတ်မိနေဖို့ ကျွန်တော်တို့ မျှော်လင့်မထားပါဘူး။ ဒါပေမယ့် သူတို့ရဲ့ ကွဲပြားမှုနဲ့ LLM ရဲ့ စာသား ဖန်တီးထုတ်လုပ်မှုမှာ သူတို့ရဲ့ အခန်းကဏ္ဍကို နားလည်ထားဖို့က အရေးကြီးပါတယ်။ Special Tokens တွေအကြောင်း ပိုမိုသိရှိလိုပါက Model ရဲ့ Hub Repository မှာရှိတဲ့ Configuration ကို ကြည့်ရှုနိုင်ပါတယ်။ ဥပမာအားဖြင့်၊ SmolLM2 Model ရဲ့ Special Tokens တွေကို [tokenizer_config.json](https://huggingface.co/HuggingFaceTB/SmolLM2-135M-Instruct/blob/main/tokenizer_config.json) မှာ ရှာတွေ့နိုင်ပါတယ်။

## နောက်ထပ် Token ခန့်မှန်းခြင်းကို နားလည်ခြင်း (Understanding next token prediction)

LLM တွေကို **Autoregressive** လို့ ခေါ်ဆိုကြပါတယ်။ ဆိုလိုတာက **တစ်ကြိမ် ထုတ်ပေးလိုက်တဲ့ ရလဒ်ဟာ နောက်တစ်ကြိမ်အတွက် ထည့်သွင်းမှု (Input) ဖြစ်လာတယ်** ဆိုတဲ့ သဘောပါပဲ။ ဒီ Loop ဟာ Model က နောက်ထပ် Token ကို EOS Token ဖြစ်မယ်လို့ ခန့်မှန်းတဲ့အထိ ဆက်လက်ဖြစ်ပေါ်နေပြီး၊ အဲဒီအခါမှာ Model က ရပ်တန့်နိုင်ပါတယ်။

<img src="https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/unit1/AutoregressionSchema.gif" alt="Visual Gif of autoregressive decoding" width="60%">

တစ်နည်းအားဖြင့်၊ LLM ဟာ EOS ကို မရောက်မချင်း စာသားကို Decode လုပ်နေပါမယ်။ ဒါပေမယ့် Decoding Loop တစ်ခုအတွင်းမှာ ဘာတွေ ဖြစ်ပျက်နေသလဲ?

Agent တွေကို လေ့လာတဲ့ ရည်ရွယ်ချက်အတွက် ဒီလုပ်ငန်းစဉ်တစ်ခုလုံးဟာ နည်းပညာအရ အနည်းငယ် ရှုပ်ထွေးနိုင်ပေမယ့်၊ အကျဉ်းချုပ် ခြုံငုံသုံးသပ်ချက်ကို ဖော်ပြပေးပါမယ်။

*   Input စာသားကို **Tokenization** လုပ်ပြီးတာနဲ့၊ Model ဟာ Input Sequence ထဲက Token တစ်ခုစီရဲ့ အဓိပ္ပာယ်နဲ့ နေရာ အချက်အလက်တွေကို ဖမ်းယူထားတဲ့ Sequence ရဲ့ ကိုယ်စားပြုမှုကို တွက်ချက်ပါတယ်။
*   ဒီ ကိုယ်စားပြုမှုဟာ Model ထဲကို ဝင်ရောက်သွားပြီး၊ Model က ၎င်းရဲ့ ဝေါဟာရစာရင်းထဲက Token တစ်ခုစီဟာ Sequence ရဲ့ နောက်ထပ် Token ဖြစ်လာနိုင်ခြေကို အဆင့်သတ်မှတ်တဲ့ Scores တွေကို ထုတ်ပေးပါတယ်။

<img src="https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/unit1/DecodingFinal.gif" alt="Visual Gif of decoding" width="60%">

ဒီ Scores တွေကို အခြေခံပြီး၊ စာကြောင်းကို ပြီးမြောက်စေဖို့အတွက် Token တွေကို ရွေးချယ်ဖို့ နည်းဗျူဟာ အမျိုးမျိုး ရှိပါတယ်။

*   အလွယ်ဆုံး Decoding နည်းဗျူဟာကတော့ အမြဲတမ်း Score အများဆုံးရတဲ့ Token ကို ယူခြင်းပဲ ဖြစ်ပါတယ်။

အောက်ပါ Space မှာ SmolLM2 နဲ့ Decoding လုပ်ငန်းစဉ်ကို သင်ကိုယ်တိုင် စမ်းသပ်ကြည့်နိုင်ပါတယ်။ (ဒီ Model အတွက် EOS Token က **`<|im_end|>`** ဖြစ်တယ်ဆိုတာကို သတိရပါ)။

<iframe
	src="https://agents-course-decoding-visualizer.hf.space"
	frameborder="0"
	width="850"
	height="450"
></iframe>

*   ဒါပေမယ့် ပိုမို အဆင့်မြင့်တဲ့ Decoding နည်းဗျူဟာတွေလည်း ရှိပါသေးတယ်။ ဥပမာအားဖြင့်၊ *Beam Search* ဟာ စုစုပေါင်း Score အများဆုံးရတဲ့ Sequence ကို ရှာဖွေဖို့အတွက် ဖြစ်နိုင်ခြေရှိတဲ့ Candidate Sequence များစွာကို စူးစမ်းရှာဖွေပါတယ်။ (တစ်ဦးချင်း Token အချို့က Score နည်းနေရင်တောင်မှ ဖြစ်ပါတယ်)။

<iframe
	src="https://agents-course-beam-search-visualizer.hf.space"
	frameborder="0"
	width="850"
	height="450"
></iframe>

Decoding အကြောင်း ပိုမိုသိရှိလိုပါက [NLP Course](https://huggingface.co/learn/nlp-course) ကို ကြည့်ရှုနိုင်ပါတယ်။

## Attention သည် အရာအားလုံးဖြစ်သည် (Attention is all you need)

Transformer Architecture ရဲ့ အဓိက အစိတ်အပိုင်းတစ်ခုကတော့ **Attention** ပဲ ဖြစ်ပါတယ်။ နောက်ထပ် စကားလုံးကို ခန့်မှန်းတဲ့အခါ၊ စာကြောင်းတစ်ခုထဲက စကားလုံးတိုင်းဟာ အရေးပါမှု တူညီခြင်း မရှိပါဘူး။ ဥပမာ *"The capital of France is ..."* ဆိုတဲ့ စာကြောင်းမှာ "France" နဲ့ "capital" လို စကားလုံးတွေက အဓိပ္ပာယ်အများဆုံးကို သယ်ဆောင်ထားပါတယ်။

<img src="https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/unit1/AttentionSceneFinal.gif" alt="Visual Gif of Attention" width="60%">
နောက်ထပ် Token ကို ခန့်မှန်းဖို့အတွက် အရေးအကြီးဆုံး စကားလုံးတွေကို ဖော်ထုတ်တဲ့ ဒီလုပ်ငန်းစဉ်ဟာ အလွန်ထိရောက်ကြောင်း သက်သေပြခဲ့ပြီး ဖြစ်ပါတယ်။

LLM တွေရဲ့ အခြေခံ သဘောတရား (နောက်ထပ် Token ကို ခန့်မှန်းခြင်း) ဟာ GPT-2 ကတည်းက မပြောင်းလဲဘဲ ရှိနေပေမယ့်၊ Neural Network တွေကို ချဲ့ထွင်ခြင်းနဲ့ Attention Mechanism ကို ပိုမိုရှည်လျားတဲ့ Sequence တွေအတွက် အလုပ်လုပ်အောင် လုပ်ဆောင်ခြင်းမှာ သိသာထင်ရှားတဲ့ တိုးတက်မှုတွေ ရှိခဲ့ပါတယ်။

LLM တွေနဲ့ ထိတွေ့ဆက်ဆံဖူးတယ်ဆိုရင်၊ LLM က စီမံဆောင်ရွက်နိုင်တဲ့ အများဆုံး Token အရေအတွက်နဲ့ ၎င်းရဲ့ အများဆုံး *Attention Span* ကို ရည်ညွှန်းတဲ့ *Context Length* ဆိုတဲ့ ဝေါဟာရနဲ့ ရင်းနှီးပြီးသား ဖြစ်ပါလိမ့်မယ်။

## LLM ကို Prompt ပေးခြင်းသည် အရေးကြီးသည် (Prompting the LLM is important)

LLM ရဲ့ တစ်ခုတည်းသော အလုပ်က Input Token တိုင်းကို ကြည့်ပြီး နောက်ထပ် Token ကို ခန့်မှန်းဖို့၊ ပြီးတော့ ဘယ် Token တွေက "အရေးကြီးတယ်" ဆိုတာကို ရွေးချယ်ဖို့ ဖြစ်တယ်ဆိုတာကို ထည့်သွင်းစဉ်းစားတဲ့အခါ၊ သင်ပေးပို့တဲ့ Input Sequence ရဲ့ စကားလုံး အသုံးအနှုန်းဟာ အလွန်အရေးကြီးပါတယ်။

LLM ကို သင်ပေးပို့တဲ့ Input Sequence ကို **Prompt** လို့ ခေါ်ပါတယ်။ Prompt ကို ဂရုတစိုက် ဒီဇိုင်းဆွဲခြင်းက **LLM ရဲ့ ထုတ်လုပ်မှုကို လိုချင်တဲ့ ရလဒ်ဆီသို့ လမ်းညွှန်ပေးဖို့** ပိုမိုလွယ်ကူစေပါတယ်။

## LLM တွေကို ဘယ်လို လေ့ကျင့်ပေးသလဲ?

LLM တွေကို စာသား ဒေတာအမြောက်အမြားနဲ့ လေ့ကျင့်ပေးထားပါတယ်။ ၎င်းတို့ဟာ **Self-supervised** သို့မဟုတ် **Masked Language Modeling** ရည်ရွယ်ချက်မှတစ်ဆင့် Sequence တစ်ခုရဲ့ နောက်ထပ် စကားလုံးကို ခန့်မှန်းဖို့ သင်ယူကြပါတယ်။

ဒီ Unsupervised Learning (ဆရာမပါဘဲ သင်ယူခြင်း) ကနေ Model ဟာ ဘာသာစကားရဲ့ ဖွဲ့စည်းပုံနဲ့ **စာသားထဲက အခြေခံ ပုံစံများကို သင်ယူပြီး၊ Model ကို အသစ်သော ဒေတာများ (Unseen Data) ကိုပါ အထွေထွေပြုနိုင်စေပါတယ် (Generalize)**။

ဒီ ကနဦး *Pre-training* ပြီးနောက်၊ LLM တွေကို သီးခြားလုပ်ငန်းများ လုပ်ဆောင်နိုင်ဖို့ Supervised Learning (ဆရာပါ သင်ယူခြင်း) ရည်ရွယ်ချက်နဲ့ **Fine-tuning** လုပ်နိုင်ပါတယ်။ ဥပမာအားဖြင့်၊ အချို့ Model တွေကို စကားပြောဆိုမှု ပုံစံများ သို့မဟုတ် Tool အသုံးပြုမှုအတွက် လေ့ကျင့်ပေးထားပြီး၊ အချို့ကတော့ အမျိုးအစားခွဲခြားခြင်း သို့မဟုတ် Code ဖန်တီးခြင်းကို အာရုံစိုက်ပါတယ်။

## LLM တွေကို ဘယ်လို အသုံးပြုနိုင်မလဲ?

သင့်မှာ အဓိက ရွေးချယ်စရာ (၂) ခု ရှိပါတယ်။

၁။ **Local မှာ Run ခြင်း** (သင့်မှာ လုံလောက်တဲ့ Hardware ရှိရင်)။

၂။ **Cloud/API ကို အသုံးပြုခြင်း** (ဥပမာ - Hugging Face Serverless Inference API မှတစ်ဆင့်)။

ဒီ Course တစ်လျှောက်လုံးမှာတော့ ကျွန်တော်တို့ Hugging Face Hub ပေါ်က API များမှတစ်ဆင့် Model များကို အဓိက အသုံးပြုသွားပါမယ်။ နောက်ပိုင်းမှာတော့ ဒီ Model တွေကို သင့်ရဲ့ Hardware မှာ Local အနေနဲ့ ဘယ်လို Run ရမလဲဆိုတာကို လေ့လာသွားပါမယ်။

## LLM တွေကို AI Agent တွေမှာ ဘယ်လို အသုံးပြုသလဲ?

LLM တွေဟာ AI Agent တွေရဲ့ အဓိက အစိတ်အပိုင်းတစ်ခုဖြစ်ပြီး၊ **လူသားဘာသာစကားကို နားလည်ပြီး ဖန်တီးထုတ်လုပ်ဖို့အတွက် အခြေခံအုတ်မြစ်ကို ပံ့ပိုးပေးပါတယ်**။

၎င်းတို့ဟာ အသုံးပြုသူရဲ့ ညွှန်ကြားချက်တွေကို နားလည်နိုင်ခြင်း၊ စကားပြောဆိုမှုရဲ့ Context ကို ထိန်းသိမ်းထားနိုင်ခြင်း၊ Plan တစ်ခုကို သတ်မှတ်နိုင်ခြင်းနဲ့ ဘယ် Tool တွေကို အသုံးပြုရမယ်ဆိုတာကို ဆုံးဖြတ်နိုင်ခြင်းတို့ လုပ်ဆောင်နိုင်ပါတယ်။

ဒီအဆင့်တွေကို ဒီ Unit မှာ ပိုမိုအသေးစိတ် လေ့လာသွားပါမယ်။ ဒါပေမယ့် အခုလောလောဆယ် သင်နားလည်ထားဖို့ လိုတာကတော့ LLM ဟာ **Agent ရဲ့ ဦးနှောက်** ဖြစ်တယ်ဆိုတာပါပဲ။

---

ဒါတွေက အချက်အလက် အများကြီးပါပဲ! ကျွန်တော်တို့ LLM တွေဆိုတာ ဘာလဲ၊ ဘယ်လို အလုပ်လုပ်သလဲ၊ ပြီးတော့ AI Agent တွေကို စွမ်းအားဖြည့်ပေးရာမှာ သူတို့ရဲ့ အခန်းကဏ္ဍကို အခြေခံအားဖြင့် လေ့လာခဲ့ပြီးပါပြီ။

ဘာသာစကား Model တွေနဲ့ သဘာဝ ဘာသာစကား စီမံဆောင်ရွက်ခြင်း (NLP) ရဲ့ စိတ်ဝင်စားစရာ ကမ္ဘာထဲကို ပိုမိုနက်နက်နဲနဲ ဝင်ရောက်ချင်တယ်ဆိုရင်တော့ ကျွန်တော်တို့ရဲ့ [အခမဲ့ NLP Course](https://huggingface.co/learn/nlp-course/chapter1/1) ကို ကြည့်ရှုဖို့ မတွန့်ဆုတ်ပါနဲ့။

LLM တွေ ဘယ်လို အလုပ်လုပ်တယ်ဆိုတာ နားလည်ပြီးတဲ့နောက်မှာ၊ **LLM တွေက သူတို့ရဲ့ ထုတ်လုပ်မှုတွေကို စကားပြောဆိုမှု ပုံစံ (Conversational Context) မှာ ဘယ်လို ဖွဲ့စည်းသလဲ** ဆိုတာကို ကြည့်ဖို့ အချိန်တန်ပါပြီ။

[ဒီ Notebook](https://huggingface.co/agents-course/notebooks/blob/main/unit1/dummy_agent_library.ipynb) ကို Run ဖို့အတွက်၊ [https://hf.co/settings/tokens](https://hf.co/settings/tokens) ကနေ ရယူနိုင်တဲ့ **Hugging Face Token တစ်ခု လိုအပ်ပါတယ်**။

Jupyter Notebook တွေကို ဘယ်လို Run ရမလဲဆိုတာကို ပိုမိုသိရှိလိုပါက [Hugging Face Hub ပေါ်ရှိ Jupyter Notebooks](https://huggingface.co/docs/hub/notebooks) ကို ကြည့်ရှုနိုင်ပါတယ်။

ဒါ့အပြင် [Meta Llama Models](https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct) ကို အသုံးပြုခွင့် တောင်းခံဖို့လည်း လိုအပ်ပါတယ်။