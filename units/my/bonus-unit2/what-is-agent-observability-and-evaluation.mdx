## AI Agent များ၏ စောင့်ကြည့်နိုင်စွမ်းနှင့် အကဲဖြတ်ခြင်း (Observability and Evaluation)

## 🔎 စောင့်ကြည့်နိုင်စွမ်း (Observability) ဆိုတာ ဘာလဲ?

**စောင့်ကြည့်နိုင်စွမ်း (Observability)** ဆိုတာကတော့ သင့်ရဲ့ AI Agent အတွင်းမှာ ဘာတွေဖြစ်ပျက်နေသလဲဆိုတာကို **ပြင်ပ အချက်ပြမှုများ** (ဥပမာ- မှတ်တမ်းများ (logs)၊ တိုင်းတာချက်များ (metrics) နဲ့ ခြေရာခံမှုများ (traces)) ကို ကြည့်ရှုပြီး နားလည်အောင် လုပ်ဆောင်ခြင်းပဲ ဖြစ်ပါတယ်။ AI Agent တွေအတွက်ဆိုရင်၊ ဒါဟာ Agent ရဲ့ လုပ်ဆောင်ချက်တွေ၊ Tool အသုံးပြုမှုတွေ၊ Model ခေါ်ဆိုမှုတွေနဲ့ တုံ့ပြန်မှုတွေကို ခြေရာခံပြီး Agent ရဲ့ စွမ်းဆောင်ရည်ကို စစ်ဆေးဖို့နဲ့ ပိုမိုကောင်းမွန်အောင် လုပ်ဆောင်ဖို့ကို ဆိုလိုပါတယ်။

![Observability dashboard](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/bonus-unit2/langfuse-dashboard.png)

## 🔭 Agent စောင့်ကြည့်နိုင်စွမ်းက ဘာကြောင့် အရေးကြီးတာလဲ?

စောင့်ကြည့်နိုင်စွမ်း မရှိဘူးဆိုရင် AI Agent တွေဟာ **"Black Boxes" (အတွင်းမှာ ဘာတွေဖြစ်နေလဲ မသိနိုင်တဲ့ သေတ္တာနက်များ)** လို ဖြစ်နေပါလိမ့်မယ်။ စောင့်ကြည့်နိုင်စွမ်း Tool တွေက Agent တွေကို ပွင့်လင်းမြင်သာစေပြီး၊ အောက်ပါအချက်တွေကို လုပ်ဆောင်နိုင်စေပါတယ်။

*   ကုန်ကျစရိတ်နဲ့ တိကျမှု (Accuracy) အကြား ကုန်သွယ်မှုတွေကို နားလည်နိုင်ခြင်း။
*   တုံ့ပြန်မှု ကြာချိန် (Latency) ကို တိုင်းတာနိုင်ခြင်း။
*   အန္တရာယ်ရှိတဲ့ ဘာသာစကားနဲ့ Prompt Injection တွေကို ထောက်လှမ်းနိုင်ခြင်း။
*   အသုံးပြုသူရဲ့ တုံ့ပြန်ချက် (User Feedback) တွေကို စောင့်ကြည့်နိုင်ခြင်း။

တစ်နည်းအားဖြင့် ပြောရရင်၊ စောင့်ကြည့်နိုင်စွမ်းက သင့်ရဲ့ Demo Agent ကို **လက်တွေ့ အသုံးပြုဖို့ (Production) အသင့်ဖြစ်စေပါတယ်**။

## 🔨 စောင့်ကြည့်နိုင်စွမ်း Tool များ

AI Agent များအတွက် အသုံးများတဲ့ စောင့်ကြည့်နိုင်စွမ်း Tool တွေထဲမှာ [Langfuse](https://langfuse.com) နဲ့ [Arize](https://www.arize.com) လို Platform တွေ ပါဝင်ပါတယ်။ ဒီ Tool တွေက အသေးစိတ် ခြေရာခံမှုများ (Traces) ကို စုဆောင်းပေးပြီး၊ Metrics တွေကို Real-time နဲ့ စောင့်ကြည့်နိုင်တဲ့ Dashboard တွေကို ပေးပါတယ်။ ဒါကြောင့် ပြဿနာတွေကို အလွယ်တကူ ရှာဖွေပြီး စွမ်းဆောင်ရည်ကို အကောင်းဆုံးဖြစ်အောင် လုပ်ဆောင်နိုင်ပါတယ်။

စောင့်ကြည့်နိုင်စွမ်း Tool တွေဟာ ၎င်းတို့ရဲ့ လုပ်ဆောင်နိုင်စွမ်းတွေမှာ အများကြီး ကွဲပြားပါတယ်။ တချို့ Tool တွေက Open Source ဖြစ်ပြီး၊ Community ရဲ့ ပံ့ပိုးမှုနဲ့ ချိတ်ဆက်မှုများစွာကို ရရှိပါတယ်။ တချို့ Tool တွေကတော့ LLMOps (LLM Operations) ရဲ့ သီးခြားကဏ္ဍများ (ဥပမာ- စောင့်ကြည့်နိုင်စွမ်း၊ အကဲဖြတ်ခြင်း သို့မဟုတ် Prompt စီမံခန့်ခွဲမှု) ကိုသာ အထူးပြုပြီး၊ တချို့ကတော့ LLMOps လုပ်ငန်းစဉ်တစ်ခုလုံးကို လွှမ်းခြုံနိုင်အောင် ဒီဇိုင်းထုတ်ထားပါတယ်။ သင့်အတွက် အသင့်တော်ဆုံး Tool ကို ရွေးချယ်နိုင်ဖို့အတွက် မတူညီတဲ့ Tool တွေရဲ့ Documentation တွေကို လေ့လာကြည့်ဖို့ ကျွန်တော်တို့ အားပေးပါတယ်။

[smolagents](https://huggingface.co/docs/smolagents/v1.12.0/en/index) လို Agent Framework များစွာဟာ စောင့်ကြည့်နိုင်စွမ်း Tool တွေကို Metadata များ ပေးပို့နိုင်ဖို့အတွက် [OpenTelemetry](https://opentelemetry.io/docs/) စံနှုန်းကို အသုံးပြုကြပါတယ်။ ဒါ့အပြင်၊ LLM တွေရဲ့ လျင်မြန်စွာ ပြောင်းလဲနေတဲ့ ကမ္ဘာမှာ ပိုမိုပြောင်းလွယ်ပြင်လွယ်ရှိစေဖို့အတွက် စောင့်ကြည့်နိုင်စွမ်း Tool တွေက Custom Instrumentation တွေကို တည်ဆောက်ကြပါတယ်။ သင်အသုံးပြုနေတဲ့ Tool ရဲ့ Documentation ကို စစ်ဆေးပြီး ဘာတွေ ပံ့ပိုးပေးထားလဲဆိုတာကို သိရှိနိုင်ပါတယ်။

## 🔬 Traces နှင့် Spans များ

စောင့်ကြည့်နိုင်စွမ်း Tool များက Agent ရဲ့ လုပ်ဆောင်မှုများကို **Traces (ခြေရာခံမှုများ)** နဲ့ **Spans (အဆင့်များ)** အဖြစ် ဖော်ပြလေ့ရှိပါတယ်။

*   **Traces (ခြေရာခံမှုများ):** အစမှ အဆုံးအထိ Agent ရဲ့ လုပ်ငန်းတစ်ခုလုံးကို ကိုယ်စားပြုပါတယ်။ (ဥပမာ - အသုံးပြုသူရဲ့ မေးခွန်းတစ်ခုကို ဖြေရှင်းပေးခြင်း)။
*   **Spans (အဆင့်များ):** Trace အတွင်းမှ တစ်ဦးချင်း လုပ်ဆောင်ချက် အဆင့်များကို ကိုယ်စားပြုပါတယ်။ (ဥပမာ - Language Model ကို ခေါ်ဆိုခြင်း သို့မဟုတ် ဒေတာ ပြန်လည်ရယူခြင်း)။

![Example of a smolagent trace in Langfuse](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/bonus-unit2/trace-tree.png)

## 📊 စောင့်ကြည့်ရမည့် အဓိက Metrics များ

စောင့်ကြည့်နိုင်စွမ်း Tool များက စောင့်ကြည့်လေ့ရှိတဲ့ အသုံးအများဆုံး Metrics များကတော့ အောက်ပါအတိုင်း ဖြစ်ပါတယ်။

**တုံ့ပြန်မှု ကြာချိန် (Latency):** Agent က ဘယ်လောက် မြန်မြန် တုံ့ပြန်သလဲ? ကြာမြင့်စွာ စောင့်ဆိုင်းရခြင်းက အသုံးပြုသူရဲ့ အတွေ့အကြုံကို ဆိုးရွားစေပါတယ်။ Agent ရဲ့ လုပ်ငန်းတစ်ခုလုံးနဲ့ တစ်ဦးချင်း အဆင့်များအတွက် Latency ကို တိုင်းတာသင့်ပါတယ်။ ဥပမာအားဖြင့်၊ Model ခေါ်ဆိုမှုများအတွက် စုစုပေါင်း စက္ကန့် ၂၀ ကြာတဲ့ Agent တစ်ခုကို ပိုမိုမြန်ဆန်တဲ့ Model ကို အသုံးပြုခြင်း သို့မဟုတ် Model ခေါ်ဆိုမှုများကို တစ်ပြိုင်နက်တည်း လုပ်ဆောင်ခြင်းဖြင့် အရှိန်မြှင့်နိုင်ပါတယ်။

**ကုန်ကျစရိတ်များ (Costs):** Agent တစ်ကြိမ် လုပ်ဆောင်မှုအတွက် ကုန်ကျစရိတ် ဘယ်လောက်ရှိသလဲ? AI Agent တွေဟာ Token အရ ကုန်ကျစရိတ် ကောက်ခံတဲ့ LLM ခေါ်ဆိုမှုများ သို့မဟုတ် ပြင်ပ API များပေါ်မှာ မှီခိုနေရပါတယ်။ Tool များကို မကြာခဏ အသုံးပြုခြင်း သို့မဟုတ် Prompt များစွာ ပေးပို့ခြင်းက ကုန်ကျစရိတ်ကို လျင်မြန်စွာ မြင့်တက်စေနိုင်ပါတယ်။ ဥပမာ - အရည်အသွေး အနည်းငယ်သာ တိုးတက်ဖို့အတွက် Agent က LLM ကို ငါးကြိမ် ခေါ်ဆိုတယ်ဆိုရင်၊ ဒီကုန်ကျစရိတ်က တန်ရဲ့လားဆိုတာကို ဆန်းစစ်ရပါမယ်။ ဒါမှမဟုတ် ခေါ်ဆိုမှု အရေအတွက်ကို လျှော့ချခြင်း သို့မဟုတ် ပိုမိုစျေးသက်သာတဲ့ Model ကို အသုံးပြုခြင်းဖြင့် ဖြေရှင်းနိုင်ပါတယ်။ Real-time စောင့်ကြည့်မှုက မမျှော်လင့်ဘဲ ကုန်ကျစရိတ် မြင့်တက်လာခြင်း (ဥပမာ - Bug များကြောင့် API Loop များ အလွန်အကျွံ ဖြစ်ပေါ်ခြင်း) ကိုလည်း ဖော်ထုတ်ပေးနိုင်ပါတယ်။

**တောင်းဆိုမှု အမှားများ (Request Errors):** Agent က ဘယ်နှစ်ကြိမ် အလုပ်မလုပ်နိုင်ခဲ့ဘူးလဲ? ဒါဟာ API အမှားများ သို့မဟုတ် Tool ခေါ်ဆိုမှု မအောင်မြင်ခြင်းများ ပါဝင်နိုင်ပါတယ်။ Production မှာ သင့် Agent ကို ပိုမို ခိုင်မာစေဖို့အတွက် Fallbacks (အရန်စနစ်) သို့မဟုတ် Retries (ပြန်လည်ကြိုးစားခြင်း) များကို သတ်မှတ်ထားနိုင်ပါတယ်။ ဥပမာ - LLM Provider A အလုပ်မလုပ်တော့ရင်၊ အရန်အဖြစ် LLM Provider B ကို ပြောင်းလဲအသုံးပြုခြင်း။

**အသုံးပြုသူ တုံ့ပြန်ချက် (User Feedback):** အသုံးပြုသူများ၏ တိုက်ရိုက် အကဲဖြတ်မှုများကို အကောင်အထည်ဖော်ခြင်းက အဖိုးတန်သော ထိုးထွင်းသိမြင်မှုများကို ပေးပါတယ်။ ဒါဟာ ရှင်းလင်းသော အဆင့်သတ်မှတ်ချက်များ (👍/👎၊ ⭐၁-၅ ပွင့်) သို့မဟုတ် စာသားမှတ်ချက်များ ပါဝင်နိုင်ပါတယ်။ အဆက်မပြတ် အနုတ်လက္ခဏာဆောင်သော တုံ့ပြန်ချက်များ ရရှိခြင်းက Agent သည် မျှော်လင့်ထားသည့်အတိုင်း အလုပ်မလုပ်ကြောင်း သတိပေးချက် ဖြစ်ပါတယ်။

**သွယ်ဝိုက်သော အသုံးပြုသူ တုံ့ပြန်ချက် (Implicit User Feedback):** အသုံးပြုသူများ၏ အပြုအမူများသည် ရှင်းလင်းသော အဆင့်သတ်မှတ်ချက်များ မရှိသော်လည်း သွယ်ဝိုက်သော တုံ့ပြန်ချက်များကို ပေးပါတယ်။ ဒါဟာ ချက်ချင်း မေးခွန်း ပြန်လည်မေးမြန်းခြင်း၊ ထပ်ခါတလဲလဲ မေးမြန်းခြင်း သို့မဟုတ် Retry ခလုတ်ကို နှိပ်ခြင်းများ ပါဝင်နိုင်ပါတယ်။ ဥပမာ - အသုံးပြုသူများက တူညီတဲ့ မေးခွန်းကို ထပ်ခါတလဲလဲ မေးနေတာကို တွေ့ရရင်၊ Agent က မျှော်လင့်ထားတဲ့အတိုင်း အလုပ်မလုပ်ဘူးဆိုတဲ့ လက္ခဏာ ဖြစ်ပါတယ်။

**တိကျမှု (Accuracy):** Agent က မှန်ကန်သော သို့မဟုတ် လိုချင်သော ရလဒ်များကို ဘယ်လောက် မကြာခဏ ထုတ်ပေးသလဲ? တိကျမှု၏ အဓိပ္ပာယ်ဖွင့်ဆိုချက်များက ကွဲပြားနိုင်ပါတယ် (ဥပမာ - ပြဿနာဖြေရှင်းမှု မှန်ကန်ခြင်း၊ အချက်အလက် ပြန်လည်ရယူမှု တိကျခြင်း၊ အသုံးပြုသူ စိတ်ကျေနပ်မှု)။ ပထမဆုံး အဆင့်ကတော့ သင့် Agent အတွက် အောင်မြင်မှုဆိုတာ ဘာလဲဆိုတာကို သတ်မှတ်ဖို့ပါပဲ။ တိကျမှုကို Automated Check များ၊ Evaluation Score များ သို့မဟုတ် Task ပြီးမြောက်မှု Label များမှတစ်ဆင့် ခြေရာခံနိုင်ပါတယ်။ ဥပမာ - Traces များကို "အောင်မြင်သည်" သို့မဟုတ် "မအောင်မြင်သည်" ဟု မှတ်သားခြင်း။

**အလိုအလျောက် အကဲဖြတ် Metrics များ (Automated Evaluation Metrics):** သင်သည် အလိုအလျောက် အကဲဖြတ်မှုများကိုလည်း သတ်မှတ်နိုင်ပါတယ်။ ဥပမာ - Agent ရဲ့ ရလဒ်က အထောက်အကူပြုသလား၊ တိကျသလားဆိုတာကို အမှတ်ပေးဖို့ LLM ကို အသုံးပြုနိုင်ပါတယ်။ Agent ရဲ့ မတူညီတဲ့ ကဏ္ဍများကို အမှတ်ပေးဖို့ ကူညီပေးတဲ့ Open Source Libraries များစွာလည်း ရှိပါတယ်။ ဥပမာ - RAG Agent များအတွက် [RAGAS](https://docs.ragas.io/) သို့မဟုတ် အန္တရာယ်ရှိသော ဘာသာစကား သို့မဟုတ် Prompt Injection များကို ထောက်လှမ်းရန် [LLM Guard](https://llm-guard.com/) တို့ ဖြစ်ပါတယ်။

လက်တွေ့မှာတော့ ဒီ Metrics တွေအားလုံးကို ပေါင်းစပ်အသုံးပြုခြင်းက AI Agent ရဲ့ ကျန်းမာရေးကို အကောင်းဆုံး လွှမ်းခြုံနိုင်ပါတယ်။ ဒီအခန်းရဲ့ [ဥပမာ Notebook](https://colab.research.google.com/#fileId=https://huggingface.co/agents-course/notebooks/blob/main/bonus-unit2/monitoring-and-evaluating-agents.ipynb) မှာ ဒီ Metrics တွေဟာ လက်တွေ့ ဥပမာတွေမှာ ဘယ်လိုပုံစံရှိသလဲဆိုတာကို ပြသပေးပါမယ်။ ဒါပေမယ့် အရင်ဆုံး၊ ပုံမှန် အကဲဖြတ်မှု လုပ်ငန်းစဉ် (Evaluation Workflow) က ဘယ်လိုပုံစံရှိသလဲဆိုတာကို လေ့လာပါမယ်။

## 👍 AI Agent များကို အကဲဖြတ်ခြင်း (Evaluating AI Agents)

စောင့်ကြည့်နိုင်စွမ်းက ကျွန်တော်တို့ကို Metrics တွေ ပေးပါတယ်။ ဒါပေမယ့် **အကဲဖြတ်ခြင်း (Evaluation)** ဆိုတာကတော့ AI Agent တစ်ခု ဘယ်လောက် ကောင်းကောင်း လုပ်ဆောင်နေသလဲ၊ ဘယ်လို တိုးတက်အောင် လုပ်ဆောင်နိုင်သလဲဆိုတာကို ဆုံးဖြတ်ဖို့အတွက် အဲဒီဒေတာ (နဲ့ စမ်းသပ်မှုများ) ကို ခွဲခြမ်းစိတ်ဖြာတဲ့ လုပ်ငန်းစဉ်ပဲ ဖြစ်ပါတယ်။ တစ်နည်းအားဖြင့်၊ Traces နဲ့ Metrics တွေ ရရှိပြီးနောက်၊ Agent ကို ဘယ်လို ဆုံးဖြတ်ပြီး ဘယ်လို ဆုံးဖြတ်ချက်တွေ ချမလဲဆိုတာကို ဆိုလိုပါတယ်။

ပုံမှန် အကဲဖြတ်မှုဟာ အရေးကြီးပါတယ်၊ ဘာကြောင့်လဲဆိုတော့ AI Agent တွေဟာ မကြာခဏဆိုသလို မခန့်မှန်းနိုင်တဲ့ (Non-deterministic) သဘောရှိပြီး ပြောင်းလဲနိုင်ပါတယ် (Update များ သို့မဟုတ် Model အပြုအမူ လမ်းလွဲခြင်းများကြောင့်)။ အကဲဖြတ်မှု မရှိရင် သင့်ရဲ့ "Smart Agent" ဟာ သူ့အလုပ်ကို ကောင်းကောင်း လုပ်နေသလား၊ ဒါမှမဟုတ် စွမ်းဆောင်ရည် ကျဆင်းသွားသလားဆိုတာကို သင်သိနိုင်မှာ မဟုတ်ပါဘူး။

AI Agent များအတွက် အကဲဖြတ်မှု အမျိုးအစား နှစ်မျိုးရှိပါတယ်- **Online Evaluation** နဲ့ **Offline Evaluation** တို့ ဖြစ်ပါတယ်။ နှစ်ခုစလုံးက အဖိုးတန်ပြီး တစ်ခုနဲ့တစ်ခု ဖြည့်ဆည်းပေးပါတယ်။ Agent တစ်ခုကို Deploy မလုပ်ခင် အနည်းဆုံး လိုအပ်တဲ့ အဆင့်ဖြစ်တဲ့ Offline Evaluation နဲ့ အများအားဖြင့် စတင်လေ့ရှိပါတယ်။

### 🥷 Offline Evaluation (အော့ဖ်လိုင်း အကဲဖြတ်ခြင်း)

![Dataset items in Langfuse](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/bonus-unit2/example-dataset.png)

ဒါဟာ ထိန်းချုပ်ထားတဲ့ ပတ်ဝန်းကျင်မှာ Agent ကို အကဲဖြတ်ခြင်း ဖြစ်ပါတယ်။ ပုံမှန်အားဖြင့် Live User Queries များ မဟုတ်ဘဲ **Test Datasets** များကို အသုံးပြုပါတယ်။ မျှော်လင့်ထားတဲ့ ရလဒ် သို့မဟုတ် မှန်ကန်တဲ့ အပြုအမူကို သင်သိထားတဲ့ စနစ်တကျ စီမံထားတဲ့ Datasets တွေကို အသုံးပြုပြီး သင့် Agent ကို အဲဒီ Datasets တွေပေါ်မှာ Run ကြည့်ရပါတယ်။

ဥပမာ - သင်က သင်္ချာ စကားလုံးပုစ္ဆာ Agent တစ်ခု တည်ဆောက်ထားတယ်ဆိုရင်၊ အဖြေသိပြီးသား ပုစ္ဆာ ၁၀၀ ပါဝင်တဲ့ [Test Dataset](https://huggingface.co/datasets/gsm8k) တစ်ခု ရှိနိုင်ပါတယ်။ Offline Evaluation ကို Development လုပ်နေစဉ်အတွင်း မကြာခဏ လုပ်ဆောင်လေ့ရှိပါတယ် (ဒါဟာ CI/CD Pipelines ရဲ့ အစိတ်အပိုင်းလည်း ဖြစ်နိုင်ပါတယ်)။ ဒါမှ တိုးတက်မှုများကို စစ်ဆေးနိုင်ပြီး စွမ်းဆောင်ရည် ကျဆင်းခြင်းကို ကာကွယ်နိုင်ပါတယ်။ အကျိုးကျေးဇူးကတော့ ဒါဟာ **ထပ်ခါတလဲလဲ လုပ်ဆောင်နိုင်ပြီး၊ မှန်ကန်တဲ့ အဖြေ (Ground Truth) ရှိတဲ့အတွက် ရှင်းလင်းတဲ့ တိကျမှု Metrics တွေကို ရရှိနိုင်ခြင်း** ဖြစ်ပါတယ်။ သင်ဟာ User Queries တွေကို အတုယူပြီး Agent ရဲ့ တုံ့ပြန်မှုတွေကို စံပြအဖြေတွေနဲ့ တိုင်းတာနိုင်ပါတယ် သို့မဟုတ် အထက်မှာ ဖော်ပြထားတဲ့ Automated Metrics တွေကို အသုံးပြုနိုင်ပါတယ်။

Offline Evaluation ရဲ့ အဓိက စိန်ခေါ်မှုကတော့ သင့်ရဲ့ Test Dataset ဟာ ပြည့်စုံပြီး လက်ရှိအခြေအနေနဲ့ သက်ဆိုင်နေဖို့ သေချာစေဖို့ပါပဲ။ Agent ဟာ သတ်မှတ်ထားတဲ့ Test Set မှာ ကောင်းကောင်း လုပ်ဆောင်နိုင်ပေမယ့် Production မှာ လုံးဝ ကွဲပြားတဲ့ Queries တွေကို ကြုံတွေ့ရနိုင်ပါတယ်။ ဒါကြောင့် Test Set တွေကို Real-world အခြေအနေတွေကို ထင်ဟပ်စေမယ့် Edge Cases တွေနဲ့ ဥပမာအသစ်တွေနဲ့ အမြဲတမ်း Update လုပ်ထားသင့်ပါတယ်။ သေးငယ်တဲ့ "Smoke Test" Cases တွေနဲ့ ပိုကြီးတဲ့ Evaluation Sets တွေကို ရောနှောအသုံးပြုခြင်းက အသုံးဝင်ပါတယ်- သေးငယ်တဲ့ Sets တွေက အမြန်စစ်ဆေးဖို့အတွက်ဖြစ်ပြီး၊ ပိုကြီးတဲ့ Sets တွေကတော့ ပိုမိုကျယ်ပြန့်တဲ့ စွမ်းဆောင်ရည် Metrics တွေအတွက် ဖြစ်ပါတယ်။

### 🔄 Online Evaluation (အွန်လိုင်း အကဲဖြတ်ခြင်း)

ဒါဟာ Agent ကို Live, Real-world ပတ်ဝန်းကျင်မှာ အကဲဖြတ်ခြင်းကို ဆိုလိုပါတယ်။ ဆိုလိုတာက Production မှာ အမှန်တကယ် အသုံးပြုနေစဉ်အတွင်း အကဲဖြတ်ခြင်းပဲ ဖြစ်ပါတယ်။ Online Evaluation မှာ Agent ရဲ့ စွမ်းဆောင်ရည်ကို အမှန်တကယ် အသုံးပြုသူရဲ့ ထိတွေ့ဆက်ဆံမှုများပေါ်မှာ စောင့်ကြည့်ပြီး ရလဒ်များကို အဆက်မပြတ် ခွဲခြမ်းစိတ်ဖြာပါတယ်။

ဥပမာ - Live Traffic ပေါ်မှာ အောင်မြင်မှုနှုန်း၊ အသုံးပြုသူ စိတ်ကျေနပ်မှု Score များ သို့မဟုတ် အခြား Metrics များကို ခြေရာခံနိုင်ပါတယ်။ Online Evaluation ရဲ့ အားသာချက်ကတော့ **Lab Setting မှာ သင် ကြိုတင်မခန့်မှန်းနိုင်တဲ့ အရာတွေကို ဖမ်းယူနိုင်ခြင်း** ဖြစ်ပါတယ်။ အချိန်ကြာလာတာနဲ့အမျှ Model Drift (Input ပုံစံများ ပြောင်းလဲသွားတဲ့အခါ Agent ရဲ့ ထိရောက်မှု ကျဆင်းခြင်း) ကို စောင့်ကြည့်နိုင်ပြီး၊ သင့်ရဲ့ Test Data မှာ မပါဝင်တဲ့ မမျှော်လင့်ထားတဲ့ Queries တွေ သို့မဟုတ် အခြေအနေတွေကို ဖမ်းယူနိုင်ပါတယ်။ ဒါဟာ Agent ရဲ့ လက်တွေ့ကမ္ဘာမှာ ဘယ်လို အပြုအမူရှိသလဲဆိုတဲ့ စစ်မှန်တဲ့ ပုံရိပ်ကို ပေးပါတယ်။

Online Evaluation မှာ အထက်မှာ ဆွေးနွေးခဲ့တဲ့ သွယ်ဝိုက်သောနှင့် ရှင်းလင်းသော User Feedback များကို စုဆောင်းခြင်းနဲ့ Shadow Tests သို့မဟုတ် A/B Tests (Agent ရဲ့ ဗားရှင်းအသစ်ကို အဟောင်းနဲ့ ယှဉ်ပြိုင်စမ်းသပ်ခြင်း) များကို လုပ်ဆောင်ခြင်းတို့ ပါဝင်လေ့ရှိပါတယ်။ စိန်ခေါ်မှုကတော့ Live Interactions တွေအတွက် ယုံကြည်စိတ်ချရတဲ့ Labels သို့မဟုတ် Scores တွေ ရရှိဖို့ ခက်ခဲနိုင်ခြင်းပဲ ဖြစ်ပါတယ်။ သင်ဟာ User Feedback သို့မဟုတ် Downstream Metrics (ဥပမာ - အသုံးပြုသူက ရလဒ်ကို နှိပ်လိုက်သလား) ပေါ်မှာ မှီခိုရနိုင်ပါတယ်။

### 🤝 နှစ်ခုပေါင်းစပ်ခြင်း (Combining the two)

လက်တွေ့မှာတော့ အောင်မြင်တဲ့ AI Agent အကဲဖြတ်မှုဟာ **Online** နဲ့ **Offline** နည်းလမ်းတွေကို ပေါင်းစပ်ထားပါတယ်။ သင်ဟာ သတ်မှတ်ထားတဲ့ Tasks တွေပေါ်မှာ သင့် Agent ကို အရေအတွက်အရ အမှတ်ပေးဖို့ ပုံမှန် Offline Benchmark တွေကို Run နိုင်ပြီး၊ Benchmark တွေ လွဲချော်သွားတဲ့ အရာတွေကို ဖမ်းယူဖို့ Live Usage ကို အဆက်မပြတ် စောင့်ကြည့်နိုင်ပါတယ်။ ဥပမာ - Offline Tests တွေက Code-generation Agent ရဲ့ အောင်မြင်မှုနှုန်း တိုးတက်နေသလားဆိုတာကို ဖမ်းယူနိုင်ပေမယ့်၊ Online Monitoring ကတော့ အသုံးပြုသူတွေက Agent ရုန်းကန်နေရတဲ့ မေးခွန်းအမျိုးအစားအသစ်ကို စတင်မေးမြန်းနေပြီဆိုတာကို သတိပေးနိုင်ပါတယ်။ နှစ်ခုကို ပေါင်းစပ်ခြင်းက ပိုမိုခိုင်မာတဲ့ ပုံရိပ်ကို ပေးပါတယ်။

တကယ်တော့၊ Team များစွာဟာ Loop တစ်ခုကို လက်ခံကျင့်သုံးကြပါတယ်- *Offline Evaluation → Agent ဗားရှင်းအသစ်ကို Deploy လုပ်ခြင်း → Online Metrics များကို စောင့်ကြည့်ပြီး မအောင်မြင်သော ဥပမာအသစ်များကို စုဆောင်းခြင်း → ထိုဥပမာများကို Offline Test Set ထဲသို့ ထည့်သွင်းခြင်း → ထပ်ခါတလဲလဲ လုပ်ဆောင်ခြင်း*။ ဤနည်းဖြင့် အကဲဖြတ်မှုသည် အဆက်မပြတ် တိုးတက်နေပါတယ်။

## 🧑‍💻 လက်တွေ့မှာ ဘယ်လို အလုပ်လုပ်သလဲဆိုတာ ကြည့်ရအောင်

နောက်အပိုင်းမှာတော့ ကျွန်တော်တို့ရဲ့ Agent ကို စောင့်ကြည့်ပြီး အကဲဖြတ်ဖို့အတွက် Observability Tool တွေကို ဘယ်လို အသုံးပြုနိုင်သလဲဆိုတဲ့ ဥပမာတွေကို ကြည့်ရှုသွားပါမယ်။