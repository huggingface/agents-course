# Building Advanced Agentic UI with smolagents

In previous units, we explored creating basic and enhanced Agent interfaces with Gradio. Now, let's build a fully-featured Agentic UI using the smolagents framework, which provides a powerful way to visualize agent reasoning, track tool usage, and create interactive interfaces.

## Introduction to smolagents UI

The smolagents library comes with a built-in Gradio UI that makes it easy to visualize your agent's reasoning process. This interface shows:

- The agent's thought process as it reasons through problems
- Tool usage and execution logs
- Final answers and outputs
- Images and other media generated by the agent

Let's start by looking at how easy it is to launch the default UI:

```python
from smolagents import (
    load_tool,
    CodeAgent,
    HfApiModel,
    GradioUI
)

# Import tool from Hub
image_generation_tool = load_tool("m-ric/text-to-image", trust_remote_code=True)

# Initialize a model
model = HfApiModel(model_id="mistralai/Mistral-7B-Instruct-v0.2")

# Initialize the agent with the image generation tool
agent = CodeAgent(tools=[image_generation_tool], model=model)

# Launch the Gradio UI
GradioUI(agent).launch(share=True)
```

This simple code creates a complete interface:

![smolagents UI](C:\Users\Yuvi\dev\gradio-module-agents-course\agents-course\units\en\unit5\Chatpter5_1.gif)

## Understanding the smolagents UI Architecture

Now let's dive deeper into how the smolagents UI works. The main component is the `GradioUI` class, which wraps around a smolagents agent and creates a Gradio interface for it. You can find the full code [here](https://github.com/huggingface/smolagents/blob/main/src/smolagents/gradio_ui.py).

Let's examine the key components of this UI:

### 1. The GradioUI Class

The `GradioUI` class is responsible for creating and launching the Gradio interface. It takes a smolagents agent as input and provides methods for interacting with the agent and handling file uploads.

```python
class GradioUI:
    """A one-line interface to launch your agent in Gradio"""

    def __init__(self, agent: MultiStepAgent, file_upload_folder: str | None = None):
        if not _is_package_available("gradio"):
            raise ModuleNotFoundError(
                "Please install 'gradio' extra to use the GradioUI: `pip install 'smolagents[gradio]'`"
            )
        self.agent = agent
        self.file_upload_folder = file_upload_folder
        self.name = getattr(agent, "name") or "Agent interface"
        self.description = getattr(agent, "description", None)
        if self.file_upload_folder is not None:
            if not os.path.exists(file_upload_folder):
                os.mkdir(file_upload_folder)
```

The `create_app` method builds the Gradio interface with a sidebar, chat area, and text input component:

```python
def create_app(self):
    import gradio as gr

    with gr.Blocks(theme="ocean", fill_height=True) as demo:
        # Add session state to store session-specific data
        session_state = gr.State({})
        stored_messages = gr.State([])
        file_uploads_log = gr.State([])

        with gr.Sidebar():
            # Rest of the Sidebar content here...
            text_input = gr.Textbox(
                        lines=3,
                        label="Chat Message",
                        container=False,
                        placeholder="Enter your prompt here and press Shift+Enter or press the button",
                    )

        # Main chat interface
        chatbot = gr.Chatbot(
            label="Agent",
            type="messages",
            avatar_images=(
                None,
                "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/smolagents/mascot_smol.png",
            ),
            resizeable=True,
            scale=1,
        )

        # Set up event handlers...

    return demo
```

### 2. Using the Sidebar Component

One of the key features of the smolagents UI is the use of the Gradio Sidebar component for user inputs and controls. The sidebar provides a clean separation between the input controls and the chat display.

```python
with gr.Sidebar():
    gr.Markdown(
        f"# {self.name.replace('_', ' ').capitalize()}"
        "\n> This web ui allows you to interact with a `smolagents` agent that can use tools and execute steps to complete tasks."
        + (f"\n\n**Agent description:**\n{self.description}" if self.description else "")
    )

    with gr.Group():
        gr.Markdown("**Your request**", container=True)
        text_input = gr.Textbox(
            lines=3,
            label="Chat Message",
            container=False,
            placeholder="Enter your prompt here and press Shift+Enter or press the button",
        )
        submit_btn = gr.Button("Submit", variant="primary")

    # File upload controls if enabled...
```

The sidebar component is particularly useful for agent interfaces as it:
- Keeps the input controls always visible, even when the chat history gets long
- Provides a clear separation between inputs and outputs
- Creates a more intuitive interface for users familiar with chat applications like ChatGPT and Claude.

### 3. Visualizing Agent Thoughts and Tool Usage

The heart of the smolagents UI is the `pull_messages_from_step` function, which converts agent steps into Gradio ChatMessage objects. This function handles the complex task of displaying different types of agent steps (actions, planning, final answers) in a visually appealing way.

Let's take a closer look at how it processes action steps:

```python
if isinstance(step_log, ActionStep):
    # Output the step number
    step_number = f"Step {step_log.step_number}" if step_log.step_number is not None else "Step"
    yield gr.ChatMessage(role="assistant", content=f"**{step_number}**")

    # First yield the thought/reasoning from the LLM
    if hasattr(step_log, "model_output") and step_log.model_output is not None:
        # Clean up the LLM output
        model_output = step_log.model_output.strip()
        # Remove any trailing <end_code> and extra backticks
        model_output = re.sub(r"```\s*<end_code>", "```", model_output)
        model_output = re.sub(r"<end_code>\s*```", "```", model_output)
        model_output = re.sub(r"```\s*\n\s*<end_code>", "```", model_output)
        model_output = model_output.strip()
        yield gr.ChatMessage(role="assistant", content=model_output)

    # For tool calls, create a parent message
    if hasattr(step_log, "tool_calls") and step_log.tool_calls is not None:
        first_tool_call = step_log.tool_calls[0]
        used_code = first_tool_call.name == "python_interpreter"
        parent_id = f"call_{len(step_log.tool_calls)}"

        # Process tool arguments and yield a message with tool usage info
        # ...

        parent_message_tool = gr.ChatMessage(
            role="assistant",
            content=content,
            metadata={
                "title": f"ðŸ› ï¸ Used tool {first_tool_call.name}",
                "id": parent_id,
                "status": "done",
            },
        )
        yield parent_message_tool
```

This function uses several advanced features of `gr.ChatMessage` that we learned about in Unit 3:

1. **Metadata for Thought Bubbles**: It uses the `metadata` parameter with a `title` key to create collapsible sections for tool usage.

2. **Status Indicators**: The `status` key is set to "done" when a tool execution is complete.

3. **Nested Messages**: It creates parent-child relationships between messages using the `id` and `parent_id` keys.

For example, here's how execution logs are displayed as a child of the tool call:

```python
# Display execution logs if they exist
if hasattr(step_log, "observations") and (
    step_log.observations is not None and step_log.observations.strip()
):  # Only yield execution logs if there's actual content
    log_content = step_log.observations.strip()
    if log_content:
        log_content = re.sub(r"^Execution logs:\s*", "", log_content)
        yield gr.ChatMessage(
            role="assistant",
            content=f"```bash\n{log_content}\n",
            metadata={"title": "ðŸ“ Execution Logs", "status": "done"},
        )
```

### 4. Streaming Agent Responses

The `stream_to_gradio` function is responsible for running the agent and streaming its outputs as Gradio ChatMessages:

```python
def stream_to_gradio(
    agent,
    task: str,
    task_images: list | None = None,
    reset_agent_memory: bool = False,
    additional_args: Optional[dict] = None,
):
    """Runs an agent with the given task and streams the messages from the agent as gradio ChatMessages."""
    total_input_tokens = 0
    total_output_tokens = 0

    for step_log in agent.run(
        task, images=task_images, stream=True, reset=reset_agent_memory, additional_args=additional_args
    ):
        # Track tokens if model provides them
        if getattr(agent.model, "last_input_token_count", None) is not None:
            total_input_tokens += agent.model.last_input_token_count
            total_output_tokens += agent.model.last_output_token_count
            if isinstance(step_log, (ActionStep, PlanningStep)):
                step_log.input_token_count = agent.model.last_input_token_count
                step_log.output_token_count = agent.model.last_output_token_count

        for message in pull_messages_from_step(step_log):
            yield message
```

This function:
1. Runs the agent with the user's task
2. Tracks token usage
3. Processes each step log through `pull_messages_from_step`
4. Yields ChatMessage objects to the Gradio interface

The streaming approach provides a responsive experience, showing the agent's reasoning in real-time rather than making the user wait for the complete result.

## Enhancing the smolagents UI

Now let's add some enhancements to the default smolagents UI to make it even more powerful.

### Adding File Upload Capabilities

One important feature for agent interfaces is the ability to upload files for the agent to process. The smolagents framework already supports this, but let's look at how we can implement file uploads in the gradio UI using Multimodal Textbox.

First let's use the Multimodal Textbox instead of a simple textbox:

```python
text_input = gr.MultimodalTextbox(
    interactive=True,
    file_count="multiple",
    show_label=False,
    sources=["upload"],
    file_types=[".csv", "image"],
    placeholder="Enter your prompt here and press Shift+Enter or press the button"
)
```

Update the event handlers accordingly:
```python
# Updated event handlers for multimodal input
text_input.submit(
    self.log_user_message,
    [text_input, file_uploads_log],
    [stored_messages, text_input, submit_btn],
).then(self.interact_with_agent, [stored_messages, chatbot, session_state], [chatbot]).then(
    lambda: (
        gr.MultimodalTextbox(
            interactive=True,
            file_count="multiple",
            show_label=False,
            sources=["upload"],
            file_types=[".csv", "image"],
            placeholder="Enter your prompt here and press Shift+Enter or press the button"
        ),
        gr.Button(interactive=True),
    ),
    None,
    [text_input, submit_btn],
)

submit_btn.click(
    self.log_user_message,
    [text_input, file_uploads_log],
    [stored_messages, text_input, submit_btn],
).then(self.interact_with_agent, [stored_messages, chatbot, session_state], [chatbot]).then(
    lambda: (
        gr.MultimodalTextbox(
            interactive=True,
            file_count="multiple",
            show_label=False,
            sources=["upload"],
            file_types=[".csv", "image"],
            placeholder="Enter your prompt here and press Shift+Enter or press the button"
        ),
        gr.Button(interactive=True),
    ),
    None,
    [text_input, submit_btn],
)
```

Modify the `log_user_message` method to handle multimodal input:

```python
def log_user_message(self, multimodal_input, file_uploads_log):
    """
    Handle the multimodal input from the user
    multimodal_input will be a dict with 'text' and 'files' keys
    """
    import gradio as gr
    
    # Extract text and files from multimodal input
    text = multimodal_input.get("text", "")
    files = multimodal_input.get("files", [])
    
    # Build the message
    messages = []
    
    # Add file paths first
    for file_path in files:
        # File paths come directly in the files array
        messages.append({"role": "user", "content": {"path": file_path}})
        # Add to file uploads log if not already present
        if file_path not in file_uploads_log:
            file_uploads_log.append(file_path)
    
    # Add text message if present
    if text:
        messages.append({"role": "user", "content": text})
    
    # Add any previously uploaded files through the separate file uploader
    if len(file_uploads_log) > 0:
        text += f"\nPreviously uploaded files: {file_uploads_log}"
    
    return messages, {"text": "", "files": []}, gr.Button(interactive=False)
```

- The method now accepts multimodal input which is a dictionary with 'text' and 'files' keys.
- Processes both text and attached files
- Combines file information into the message

Now when a user submits a prompt, the file paths are appended to the prompt so the agent knows about them.

We also need to update the `interact_with_agent` method so that it can handle user prompts with file inputs robustly:

```python
def interact_with_agent(self, messages, chatbot_history, session_state):
    import gradio as gr

    # Get the agent type from the template agent
    if "agent" not in session_state:
        session_state["agent"] = self.agent

    try:
        # Add all messages to chatbot history
        for message in messages:
            chatbot_history.append(message)
        yield chatbot_history

        # Extract text and files for the agent
        text_messages = [msg["content"] for msg in messages if isinstance(msg["content"], str)]
        file_messages = [msg["content"]["path"] for msg in messages if isinstance(msg["content"], dict)]
        
        # Combine text messages and file information for the agent
        task = " ".join(text_messages)
        if file_messages:
            task += f"\nAttached files: {', '.join(file_messages)}"

        for msg in stream_to_gradio(session_state["agent"], task=task, reset_agent_memory=False):
            chatbot_history.append(msg)
            yield chatbot_history

        yield chatbot_history
    except Exception as e:
        print(f"Error in interaction: {str(e)}")
        chatbot_history.append(gr.ChatMessage(role="assistant", content=f"Error: {str(e)}"))
        yield chatbot_history
```

You can find the full updated gradio UI file [here](https://github.com/yvrjsharma/HugginFace_Gradio/blob/main/updated_smolagents_gradio_ui.py).

### Creating a Multipage Application

For more complex agent interfaces, we can extend our application with multiple pages. This allows us to include documentation, different agent configurations, or specialized interfaces for different tasks.

Here's how we can create a multipage application with our smolagents UI:

```python
import gradio as gr
from smolagents import CodeAgent, HfApiModel, GradioUI, load_tool, DuckDuckGoSearchTool

# Initialize agents
search_agent = CodeAgent(
    tools=[DuckDuckGoSearchTool()],
    model=HfApiModel(model_id="mistralai/Mistral-7B-Instruct-v0.2"),
)

# Import tool from Hub
# Initialize the agent with the image generation tool
image_agent = CodeAgent(
    tools=[load_tool("m-ric/text-to-image", trust_remote_code=True)], 
    model=HfApiModel(model_id="mistralai/Mistral-7B-Instruct-v0.2")
)

# Create UI instances
search_ui = GradioUI(search_agent)
image_ui = GradioUI(image_agent)

# Build multipage app
with gr.Blocks(theme="ocean") as demo:
    gr.Markdown("# Multiple-Page Agent Interface")
    
    # Main page
    search_ui.create_app()

# Add additional pages
with demo.route("Image Generation", "/image") as image_page:
    gr.Markdown("# Image Generation Agent")
    image_ui.create_app() 

# Documentation page
with demo.route("Documentation", "/docs") as docs_page:
    gr.Markdown("# Agent Documentation")
    gr.Markdown("""
    ## How to use this interface
    
    - **Search Agent**: Use the main page for your web search queries
    - **Image Generation**: Use the image page to create images from text descriptions
    - **Tips**: Be specific in your requests for best results
    """)

demo.launch()
```

This creates a **multipage** application with:
- A Main page with the Search Agent
- An Image Generation page with the Image Agent
- A Documentation page with helpful information

Each page has its own URL path, and Gradio automatically creates a navigation bar at the top of the interface. You can read more about creating multipage apps with Gradio [here](https://www.gradio.app/guides/multipage-apps).


### Adding Deep Links

You can add a button to your Gradio app that creates a unique URL you can use to share your app and all components as they currently are with others. 

- This is useful for sharing unique and interesting generations from your agentic application, or 
- For saving a snapshot of your app at a particular point in time.

It is very simple to add a deep link button to your app, you just need to place the `gr.DeepLinkButton` component anywhere in your app. 

Note that, for the URL to be accessible to others, your app must be available at a public URL. So be sure to host your agentic app on platforms like Hugging Face Spaces or use the `share=True` parameter when launching your agentic app.

Let's see an example of how this works: 

```python
import gradio as gr
from smolagents import CodeAgent, HfApiModel, GradioUI, load_tool

# Import tool from Hub
# Initialize the agent with the image generation tool
image_agent = CodeAgent(
    tools=[load_tool("m-ric/text-to-image", trust_remote_code=True)], 
    model=HfApiModel(model_id="mistralai/Mistral-7B-Instruct-v0.2")
)

# Create UI instance
image_ui = GradioUI(image_agent)

# Build another gradio UI
with gr.Blocks(theme="citrus") as demo:
    gr.Markdown("# Share your Agent Interfaces with Deeplinks")
    # how to
    gr.Markdown("""
    ### How to use this interface
    
    - Click the DeepLinks button
    - It copies a unique URL that you can use to share your app and all components as they currently are with others
    - Useful for sharing unique and interesting generations from your agentic applications!
    """)
    # render agent ui
    image_ui.create_app()
    gr.DeepLinkButton()

demo.launch()
```


## Styling and Theming Your Agent UI

The smolagents UI uses Gradio's theming system to create an appealing interface. By default, it uses the "ocean" theme, but you can customize this to match your branding or preferences.

To change the theme, modify the `gr.Blocks` constructor:

```python
with gr.Blocks(theme=gr.themes.Soft(), fill_height=True) as demo:
    # UI components here...
```

Gradio provides several built-in themes:
- `gr.themes.Base()` - The default theme
- `gr.themes.Soft()` - A softer color palette
- `gr.themes.Monochrome()` - A black and white theme
- `gr.themes.Glass()` - A transparent theme

You can also create custom themes by extending the `gr.Theme` class:

```python
custom_theme = gr.themes.Soft().set(
    body_background_fill="linear-gradient(to right, #4880EC, #019CAD)",
    body_background_fill_dark="linear-gradient(to right, #1e3c72, #2a5298)",
    button_primary_background_fill="#019CAD",
    button_primary_background_fill_dark="#2a5298",
)

with gr.Blocks(theme=custom_theme) as demo:
    # UI components here...
```
Read more in our Theming Guide, [here](https://www.gradio.app/guides/theming-guide).

## Summary

In this unit, we've learned how to:

1. Use the smolagents library's built-in GradioUI to visualize agent thinking and tool usage
2. Understand the architecture of the smolagents UI, particularly the `pull_messages_from_step` function
3. Enhance the UI with file upload capabilities
4. Create multipage applications using different agents with clear navigation routes
5. Apply custom themes to improve the appearance
6. Use Deep Links for sharing snapshot of your Agent's results and state

The smolagents UI demonstrates the power of Gradio for creating agent interfaces that are both highly functional and user-friendly. By visualizing the agent's thought process and tool usage, it helps users understand what the agent is doing and build trust in its capabilities.