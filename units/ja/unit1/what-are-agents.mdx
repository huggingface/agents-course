## エージェントとは何か？

<img src="https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/unit1/whiteboard-no-check.jpg" alt="Unit 1 planning"/>

このセクションが終了するまでに、エージェントの概念とそのAIにおけるさまざまな応用について理解できるようになります。

エージェントが何かを理解するために、まずはその基本的な定義から始めましょう。

## 全体像: アルフレッド・ザ・エージェント

アルフレッドに会いましょう。アルフレッドは**エージェント**です。

<img src="https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/unit1/this-is-alfred.jpg" alt="This is Alfred"/>

アルフレッドが「アルフレッド、コーヒーをお願いします」といった**コマンドを受け取る**と想像してみてください。

<img src="https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/unit1/coffee-please.jpg" alt="I would like a coffee"/>

アルフレッドは**自然言語を理解する**ため、私たちのリクエストをすぐに把握します。

注文を満たす前に、アルフレッドは**推論と計画**に取り組み、必要な手順とツールを考え出します。

1. キッチンに行く  
2. コーヒーメーカーを使う
3. コーヒーを淹れる  
4. コーヒーを持ってくる

<img src="https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/unit1/reason-and-plan.jpg" alt="Reason and plan"/>

計画ができたら、彼は**行動しなければなりません**。計画を実行するために、**彼は知っているツールのリストからツールを使用できます**。

この場合、コーヒーを作るために、彼はコーヒーメーカーを使います。彼はコーヒーメーカーを起動してコーヒーを淹れます。

<img src="https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/unit1/make-coffee.jpg" alt="Make coffee"/>

最後に、アルフレッドは淹れたてのコーヒーを私たちに持ってきます。

<img src="https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/unit1/bring-coffee.jpg" alt="Bring coffee"/>

これがエージェントの基本的な流れです： **AIモデルが自然言語を理解し、推論と計画を行い、環境と対話してアクションを実行する**。

私たちはこれをエージェントと呼びます。なぜなら、エージェントには_エージェンシー(Agency)_、つまり環境と対話する能力があるからです。

<img src="https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/unit1/process.jpg" alt="Agent process"/>

## より正式に

大局を把握したところで、より正確な定義を見てみましょう。

> エージェントとは、AIモデルを活用して環境と対話し、ユーザー定義の目的を達成するシステムです。推論、計画、およびアクションの実行（しばしば外部ツールを介して）を組み合わせてタスクを実行します。

エージェントは2つの主要な部分から成り立っていると考えてください。

1. **脳 (AI Model)**

ここではすべての思考が行われます。AIモデルは**推論と計画を担当**します。
状況に基づいて**どのアクションを取るかを決定**します。

2. **身体 (Capabilities and Tools)**

この部分は、**エージェントが装備されているすべての機能**を表します。

**可能なアクションの範囲**は、エージェントが**装備しているもの**に依存します。たとえば、人間には翼がないため、「飛ぶ」**アクション**を実行することはできませんが、「歩く」、「走る」、「ジャンプする」、「つかむ」などの**アクション**を実行することはできます。

### "エージェンシー(Agency)"とは？

この定義に従うと、エージェントはエージェンシーの増加する連続的な領域に存在します。

| エージェンシーレベル | 説明 | それが呼ばれるもの | 例 |
| --- | --- | --- | --- |
| ☆☆☆ | エージェントの出力はプログラムのフローに影響を与えない | シンプルプロセッサ | `process_llm_output(llm_response)` |
| ★☆☆ | エージェントの出力は基本的な制御フローを決定する | ルーター | `if llm_decision(): path_a() else: path_b()` |
| ★★☆ | エージェントの出力は関数の実行を決定する | ツールコーラー | `run_function(llm_chosen_tool, llm_chosen_args)` |
| ★★★ | エージェントの出力は反復とプログラムの継続を制御する | マルチステップエージェント | `while llm_should_continue(): execute_next_step()` |
| ★★★ | One agentic workflow can start another agentic workflow | Multi-Agent | `if llm_trigger(): execute_agent()` |

表は[smolagents conceptual guide](https://huggingface.co/docs/smolagents/conceptual_guides/intro_agents)からのものです。

## どのようなAIモデルをエージェントに使用するか？

最も一般的にエージェントで使用されるAIモデルは、LLM（大規模言語モデル）です。これは、**テキスト**を入力として受け取り、**テキスト**を出力します。

よく知られている例としては、**OpenAI**の**GPT4**、**Meta**の**LLama**、**Google**の**Gemini**などがあります。これらのモデルは膨大な量のテキストで訓練されており、一般化能力に優れています。LLMについては[次のセクション](what-are-llms)で詳しく学びます。

<tip>
エージェントのコアモデルとして、他の入力を受け入れるモデルを使用することも可能です。たとえば、画像を入力として理解するLLMのような**ビジョンランゲージモデル（VLM）**があります。今のところLLMに焦点を当て、後で他のオプションについて議論します。
</tip>

## AIはどのようにして環境に対して行動を起こすのか？

LLMは素晴らしいモデルですが、**テキストを生成することしかできません**。

しかし、HuggingChatやChatGPTのような有名なチャットアプリに画像を生成するように依頼すると、彼らはそれを実行できます！これはどういうことでしょうか？

その答えは、HuggingChat、ChatGPT、そして類似のアプリの開発者が、LLMが画像を生成するために使用できる追加機能（**ツール**と呼ばれる）を実装したことです。

<figure>
<img src="https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/unit1/eiffel_brocolis.jpg" alt="Eiffel Brocolis"/>
<figcaption>HuggingChatで「エッフェル塔のブロッコリーの画像を生成して」と依頼した結果。</figcaption>
</figure>

私たちは[ツール](tools)セクションでツールについてもっと学びます。

## エージェントはどのようなタスクを実行できるのか？

エージェントは、**ツール**を介して実装された任意のタスクを実行し、**アクション**を完了することができます。

たとえば、私がコンピュータ上で個人アシスタント（Siriのような）として機能するエージェントを書き、「今日の会議を延期するようにマネージャーにメールを送って」と依頼した場合、メールを送信するためのコードを提供できます。これは、エージェントがメールを送信する必要があるときに使用できる新しいツールになります。これをPythonで書くことができます。

```python
def send_message_to(recipient, message):
    """Useful to send an e-mail message to a recipient"""
    ...
```

LLMは、必要なときにツールを実行するためのコードを生成し、したがって目的のタスクを達成します。

```python
send_message_to("Manager", "Can we postpone today's meeting?")
```

ツールの設計は非常に重要であり、エージェントの品質に大きな影響を与えます。特定のタスクには、特別に作成されたツールが必要な場合がありますが、他のタスクは「web_search」のような汎用ツールで解決できるかもしれません。

> **アクションはツールとは異なる**ことに注意してください。たとえば、アクションは複数のツールを使用して完了することができます。

エージェントがその環境と対話できるようにすることは、**企業や個人の実際の使用を可能にします**。

### 例1: 個人用バーチャルアシスタント

Siri、Alexa、Googleアシスタントなどのバーチャルアシスタントは、ユーザーのデジタル環境を使用して代理で対話する際にエージェントとして機能します。

ユーザーのクエリを受け取り、コンテキストを分析し、データベースから情報を取得し、応答を提供したり、リマインダーの設定、メッセージの送信、スマートデバイスの制御などのアクションを開始したりします。

### 例2: カスタマーサービスチャットボット

多くの企業は、顧客と自然言語で対話するエージェントとしてチャットボットを展開しています。

これらのエージェントは、質問に答えたり、トラブルシューティングの手順を案内したり、内部データベースで問題を開いたり、取引を完了したりすることができます。

彼らの事前定義された目標には、ユーザー満足度の向上、待機時間の短縮、販売転換率の向上が含まれる場合があります。顧客と直接対話し、対話から学び、時間とともに応答を適応させることで、彼らはエージェントの基本原則を実証します。


### 例3: ビデオゲームにおけるAIノンプレイヤーキャラクター

LLMによって強化されたAIエージェントは、ノンプレイヤーキャラクター（NPC）をよりダイナミックで予測不可能にすることができます。

厳格な行動ツリーに従うのではなく、彼らは**文脈に応じて反応し、プレイヤーのインタラクションに適応し**、より微妙な対話を生成することができます。この柔軟性は、プレイヤーの行動に応じて進化する、より生き生きとした魅力的なキャラクターを作成するのに役立ちます。

---

最後に、エージェントはAIモデル（通常はLLM）をコア推論エンジンとして使用して、以下のことを行います：

- **自然言語を理解する:** 人間の指示を意味のある方法で解釈し、応答します。

- **推論と計画:** 情報を分析し、意思決定を行い、問題を解決するための戦略を考案します。

- **環境と対話する:** 情報を収集し、アクションを実行し、そのアクションの結果を観察します。

これでエージェントについての理解が深まったと思いますので、短い非評価クイズで確認しましょう。その後、「エージェントの脳」である[LLMs](what-are-llms)に進みます。
